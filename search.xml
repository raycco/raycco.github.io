<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AudioTrack到AudioFlinger数据传输：共享内存]]></title>
    <url>%2F2017%2F12%2F28%2FAudioTrack%E5%88%B0AudioFlinger%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[从前面AudioFlinger启动知道，其运行在audioserver进程，而创建AudioTrack一般在APP端创建，APP一般运行在单独的进程，所以这里说AudioTrack到AudioFlinger数据传输，其实也是两个进程之间数据传输，这篇笔记就记录了AudioTrack是怎么将Audio数据转移到AudioFlinger的。 通信模型在AudioTrack和AudioFlinger通信过程中，有两种情形：MODE_STATIC和MODE_STREAM。 MODE_STATIC主要针对数据量较小及延迟要求高的音频，比如短促的游戏音乐，这种情况下，共享内存的创建在Client进程，且一般也不会动态处理这段buffer，AudioTrack会一次性将数据通过共享内存传递到AudioFlinger。 MODE_STREAM适用于大多数应用场景，是Android主要的音频播放方式，如下图，就是MODE_STREAM模式下，AudioTrack到AudioFlinger数据传输的模式，同样使用共享内存，且是环形buffer，通过生产者-消费者模式进行数据传输。与MODE_STATIC的主要区别在于共享内存的创建方式及对共享内存的控制方式不太一样。 共享内存控制块对于共享内存的通信方式，涉及到两个进程对于共享内存的读写，这就会有进程间的同步，所以共享内存控制块起着关键性的作用，所以下面我们看看其定义：frameworks/av/include/private/media/AudioTrackShared.h1234567891011121314151617// for audio_track_cblk_t::mFlags#define CBLK_UNDERRUN 0x01 // set by server immediately on output underrun, cleared by client#define CBLK_FORCEREADY 0x02 // set: track is considered ready immediately by AudioFlinger, // clear: track is ready when buffer full#define CBLK_INVALID 0x04 // track buffer invalidated by AudioFlinger, need to re-create#define CBLK_DISABLED 0x08 // output track disabled by AudioFlinger due to underrun, // need to re-start. Unlike CBLK_UNDERRUN, this is not set // immediately, but only after a long string of underruns.// 0x10 unused#define CBLK_LOOP_CYCLE 0x20 // set by server each time a loop // cycle other than final one completes#define CBLK_LOOP_FINAL 0x40 // set by server when the final loop cycle completes#define CBLK_BUFFER_END 0x80 // set by server when the position // reaches end of buffer if not looping#define CBLK_OVERRUN 0x100 // set by server immediately on input overrun, cleared by client#define CBLK_INTERRUPT 0x200 // set by client on interrupt(), cleared by client in obtainBuffer()#define CBLK_STREAM_END_DONE 0x400 // set by server on render completion, cleared by client 如上是对于传输的状态的定义。 CBLK_UNDERRUN：AudioTrack写入数据的速度跟不上AudioFlinger读取数据的速度，使得 AudioFlinger不能及时获取到预期的数据量，反映到现实的后果就是声音断续；这种情况的根本原因大多是应用程序不能及时写入数据或者缓冲区分配过小，AudioTrack本身并没有错；AudioFlinger针对这点做了容错处理：当发现underrun时，先陷入短时间的睡眠，不急着读取数据，让应用程序准备更多的数据。 CBLK_OVERRUN：刚好和CBLK_UNDERRUN相反，主要对于AudioRecord，底层写数据的速度太快，AudioRecord读取数据比较慢，数据传输就会处于超负荷的运行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Important: do not add any virtual methods, including ~struct audio_track_cblk_t&#123; // Since the control block is always located in shared memory, this constructor // is only used for placement new(). It is never used for regular new() or stack. audio_track_cblk_t(); /*virtual*/ ~audio_track_cblk_t() &#123; &#125; ...... // friend classes of Proxy // The data members are grouped so that members accessed frequently and in the same context // are in the same line of data cache. uint32_t mServer; // Number of filled frames consumed by server (mIsOut), // or filled frames provided by server (!mIsOut). // It is updated asynchronously by server without // a barrier. The value should be used // "for entertainment purposes only", // which means don't make important decisions based on it. volatile int32_t mFutex; // event flag: down (P) by client, // up (V) by server or binderDied() or interrupt()#define CBLK_FUTEX_WAKE 1 // if event flag bit is set, then a deferred wake is pendingprivate: // This field should be a size_t, but since it is located in shared // memory we force to 32-bit. The client and server may have // different typedefs for size_t. uint32_t mMinimum; // server wakes up client if available &gt;= mMinimum // Stereo gains for AudioTrack only, not used by AudioRecord. gain_minifloat_packed_t mVolumeLR; uint32_t mSampleRate; // AudioTrack only: client's requested // sample rate in Hz or 0 == default. // Write-only client, read-only server. PlaybackRateQueue::Shared mPlaybackRateQueue; // client write-only, server read-only uint16_t mSendLevel; // Fixed point U4.12 so 0x1000 means 1.0 // server write-only, client read ExtendedTimestampQueue::Shared mExtendedTimestampQueue; // This is set by AudioTrack.setBufferSizeInFrames(). // A write will not fill the buffer above this limit. volatile uint32_t mBufferSizeInFrames; // effective size of the bufferpublic: volatile int32_t mFlags; // combinations of CBLK_*public: union &#123; AudioTrackSharedStreaming mStreaming; AudioTrackSharedStatic mStatic; int mAlign[8]; &#125; u; // Cache line boundary (32 bytes)&#125;; 我们比较关注的mFutex，用于进程的同步，mMinimum唤醒server最小值，mFlags标记当前传输处于什么样的状态，没有直接看到环形buffer相关控制变量，但是有一个联合体。 对于MODE_STREAM：123456789101112struct AudioTrackSharedStreaming &#123; // similar to NBAIO MonoPipe // in continuously incrementing frame units, take modulo buffer size, // which must be a power of 2 volatile int32_t mFront; // read by consumer (output: server, input: client) volatile int32_t mRear; // written by producer (output: client, input: server) volatile int32_t mFlush; // incremented by client to indicate a request to flush; // server notices and discards all data between mFront and mRear volatile uint32_t mUnderrunFrames; // server increments for each unavailable // but desired frame volatile uint32_t mUnderrunCount; // server increments for each underrun occurrence&#125;; 共享内存的创建时序图首先通过时序图看看匿名共享内存的创建过程，对于MODE_STREAM，匿名共享内创建在AudioFlinger中，即在audioserver中，其中会通过Binder通信将创建好的共享内存随Track的返回而返回到AudioTrack的client中，通过获取共享内存地址再重新映射到client进程，这样两个进程就可以操作这块内存传输数据了。 源代码看完时序图，大致看看源代码，看看其创建过程。123456789101112status_t AudioTrack::set( ...... const sp&lt;IMemory&gt;&amp; sharedBuffer, ......)&#123; ...... mSharedBuffer = sharedBuffer; ...... // create the IAudioTrack status_t status = createTrack_l(); ......&#125; 在创建AudioTrack的时候，有一个比较重要的参数sharedBuffer，这个参数决定了创建共享内存的进程，若为空，则会在AudioFlinger创建，否则在创建AudioTrack之前就创建。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364status_t AudioTrack::createTrack_l()&#123; const sp&lt;IAudioFlinger&gt;&amp; audioFlinger = AudioSystem::get_audio_flinger(); ...... audio_io_handle_t output; audio_stream_type_t streamType = mStreamType; ...... status = AudioSystem::getOutputForAttr(attr, &amp;output, mSessionId, &amp;streamType, mClientUid, &amp;config, mFlags, mSelectedDeviceId, &amp;mPortId); ...... sp&lt;IAudioTrack&gt; track = audioFlinger-&gt;createTrack(streamType, mSampleRate, mFormat, mChannelMask, &amp;temp, &amp;flags, mSharedBuffer, output, mClientPid, tid, &amp;mSessionId, mClientUid, &amp;status, mPortId); ...... sp&lt;IMemory&gt; iMem = track-&gt;getCblk(); // 这里获取整个匿名共享内存IMemory ...... void *iMemPointer = iMem-&gt;pointer(); // 映射匿名共享内存到当前进程，获取共享内存首地址 ...... mAudioTrack = track; // 保存AudioFlinger::PlaybackThread::Track的代理对象 IAudioTrack mCblkMemory = iMem; // 保存匿名共享内存 // 控制块位于匿名共享内存的首部 audio_track_cblk_t* cblk = static_cast&lt;audio_track_cblk_t*&gt;(iMemPointer); mCblk = cblk; ...... void* buffers; if (mSharedBuffer == 0) &#123; buffers = cblk + 1; &#125; else &#123; buffers = mSharedBuffer-&gt;pointer(); if (buffers == NULL) &#123; ALOGE("Could not get buffer pointer"); return NO_INIT; &#125; &#125; ...... mServer = 0; // update proxy if (mSharedBuffer == 0) &#123; mStaticProxy.clear(); // 当mSharedBuffer为空，意味着音轨数据模式为MODE_STREAM，那么创建 // AudioTrackClientProxy对象 mProxy = new AudioTrackClientProxy(cblk, buffers, frameCount, mFrameSize); &#125; else &#123; // 当mSharedBuffer非空，意味着音轨数据模式为MODE_STATIC，那么创建 // StaticAudioTrackClientProxy对象 mStaticProxy = new StaticAudioTrackClientProxy(cblk, buffers, frameCount, mFrameSize); mProxy = mStaticProxy; &#125; ......&#125; 在AudioPolicyService启动的时候在AudioPolicyService中会保存着与放音线程PlaybackThread对应的output，这里首先通过streamType等参数获取audio_io_handle_t代表的output，然后进入AudioFlinger创建IAudioTrack。通过IAudioTrack获取整个匿名共享内存IMemory，然后再映射匿名共享内存到当前进程，保存到mCblkMemory，mCblkMemory的首部是匿名共享内存控制块audio_track_cblk_t，最后创建AudioTrackClientProxy，后续通过AudioTrackClientProxy操作共享内存写数据。123456789101112131415161718192021222324252627282930313233// frameworks/av/services/audioflinger/AudioFlinger.cppsp&lt;IAudioTrack&gt; AudioFlinger::createTrack( ...... const sp&lt;IMemory&gt;&amp; sharedBuffer, audio_io_handle_t output, ......)&#123; sp&lt;PlaybackThread::Track&gt; track; sp&lt;TrackHandle&gt; trackHandle; sp&lt;Client&gt; client; ...... &#123; Mutex::Autolock _l(mLock); // 根据传入来的audio_io_handle_t，找到对应的PlaybackThread PlaybackThread *thread = checkPlaybackThread_l(output); if (thread == NULL) &#123; ALOGE("no playback thread found for output handle %d", output); lStatus = BAD_VALUE; goto Exit; &#125; client = registerPid(pid); ...... track = thread-&gt;createTrack_l(client, streamType, sampleRate, format, channelMask, frameCount, sharedBuffer, lSessionId, flags, tid, clientUid, &amp;lStatus, portId); ...... &#125; // return handle to client trackHandle = new TrackHandle(track);// 创建Track的代理TrackHandle并返回Exit: *status = lStatus; return trackHandle;&#125; 在AudioFlinger中首先根据audio_io_handle_t找到相应的放音线程PlaybackThread，创建Client保存对应的客户端，在PlaybackThread中创建Track，最后创建Track的代理TrackHandle并返回到AudioTrack。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// frameworks/av/services/audioflinger/Threads.cppsp&lt;AudioFlinger::PlaybackThread::Track&gt; AudioFlinger::PlaybackThread::createTrack_l( const sp&lt;AudioFlinger::Client&gt;&amp; client, ...... const sp&lt;IMemory&gt;&amp; sharedBuffer, ......)&#123; size_t frameCount = *pFrameCount; sp&lt;Track&gt; track; status_t lStatus; ...... // For normal PCM streaming tracks, update minimum frame count. // For compatibility with AudioTrack calculation, buffer depth is forced // to be at least 2 x the normal mixer frame count and cover audio hardware latency. // This is probably too conservative, but legacy application code may depend on it. // If you change this calculation, also review the start threshold which is related. if (!(*flags &amp; AUDIO_OUTPUT_FLAG_FAST) &amp;&amp; audio_has_proportional_frames(format) &amp;&amp; sharedBuffer == 0) &#123; // this must match AudioTrack.cpp calculateMinFrameCount(). // TODO: Move to a common library uint32_t latencyMs = 0; lStatus = mOutput-&gt;stream-&gt;getLatency(&amp;latencyMs); if (lStatus != OK) &#123; ALOGE("Error when retrieving output stream latency: %d", lStatus); goto Exit; &#125; uint32_t minBufCount = latencyMs / ((1000 * mNormalFrameCount) / mSampleRate); if (minBufCount &lt; 2) &#123; minBufCount = 2; &#125; // For normal mixing tracks, if speed is &gt; 1.0f (normal), AudioTrack // or the client should compute and pass in a larger buffer request. size_t minFrameCount = minBufCount * sourceFramesNeededWithTimestretch( sampleRate, mNormalFrameCount, mSampleRate, AUDIO_TIMESTRETCH_SPEED_NORMAL /*speed*/); if (frameCount &lt; minFrameCount) &#123; // including frameCount == 0 frameCount = minFrameCount; &#125; &#125; ...... &#123; // scope for mLock Mutex::Autolock _l(mLock); ...... track = new Track(this, client, streamType, sampleRate, format, channelMask, frameCount, NULL, sharedBuffer, sessionId, uid, *flags, TrackBase::TYPE_DEFAULT, portId); ...... &#125; lStatus = NO_ERROR;Exit: *status = lStatus; return track;&#125; 根据HAL的参数获取延迟latencyMs，mNormalFrameCount是在放音线程PlaybackThread创建时，预先初始化的frame count，根据这两个值得出播放当前音频需要的最小的buffer count，继而的到最小的frame count，然后创建Track。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155// frameworks/av/services/audioflinger/Tracks.cppAudioFlinger::PlaybackThread::Track::Track( PlaybackThread *thread, const sp&lt;Client&gt;&amp; client, audio_stream_type_t streamType, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t frameCount, void *buffer, const sp&lt;IMemory&gt;&amp; sharedBuffer, audio_session_t sessionId, uid_t uid, audio_output_flags_t flags, track_type type, audio_port_handle_t portId) : TrackBase(thread, client, sampleRate, format, channelMask, frameCount, (sharedBuffer != 0) ? sharedBuffer-&gt;pointer() : buffer, sessionId, uid, true /*isOut*/, (type == TYPE_PATCH) ? ( buffer == NULL ? ALLOC_LOCAL : ALLOC_NONE) : ALLOC_CBLK, type, portId), mFillingUpStatus(FS_INVALID), // mRetryCount initialized later when needed mSharedBuffer(sharedBuffer), ......&#123; ...... if (sharedBuffer == 0) &#123; // 数据传输模式为MODE_STREAM模式，创建一个AudioTrackServerProxy对象 // PlaybackThread将持续使用它从环形buffer上取得可读数据的位置 mAudioTrackServerProxy = new AudioTrackServerProxy(mCblk, mBuffer, frameCount, mFrameSize, !isExternalTrack(), sampleRate); &#125; else &#123; // Is the shared buffer of sufficient size? // (frameCount * mFrameSize) is &lt;= SIZE_MAX, checked in TrackBase. if (sharedBuffer-&gt;size() &lt; frameCount * mFrameSize) &#123; // Workaround: clear out mCblk to indicate track hasn't been properly created. mCblk-&gt;~audio_track_cblk_t(); // destroy our shared-structure. if (mClient == 0) &#123; free(mCblk); &#125; mCblk = NULL; mSharedBuffer.clear(); // release shared buffer early android_errorWriteLog(0x534e4554, "38340117"); return; &#125; // 数据传输模式为MODE_STATIC模式，创建一个StaticAudioTrackServerProxy对象 mAudioTrackServerProxy = new StaticAudioTrackServerProxy(mCblk, mBuffer, frameCount, mFrameSize); &#125; mServerProxy = mAudioTrackServerProxy; ......&#125;// frameworks/av/services/audioflinger/Tracks.cppAudioFlinger::ThreadBase::TrackBase::TrackBase( ThreadBase *thread, const sp&lt;Client&gt;&amp; client, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t frameCount, void *buffer, audio_session_t sessionId, uid_t clientUid, bool isOut, alloc_type alloc, track_type type, audio_port_handle_t portId) : RefBase(), mThread(thread), mClient(client), mFrameSize(audio_has_proportional_frames(format) ? mChannelCount * audio_bytes_per_sample(format) : sizeof(int8_t)) ......&#123; ...... size_t bufferSize = buffer == NULL ? roundup(frameCount) : frameCount; bufferSize *= mFrameSize; size_t size = sizeof(audio_track_cblk_t); if (buffer == NULL &amp;&amp; alloc == ALLOC_CBLK) &#123; // check overflow when computing allocation size for streaming tracks. if (size &gt; SIZE_MAX - bufferSize) &#123; android_errorWriteLog(0x534e4554, "34749571"); return; &#125; size += bufferSize; &#125; if (client != 0) &#123; mCblkMemory = client-&gt;heap()-&gt;allocate(size); // 创建共享内存并映射当前进程 if (mCblkMemory == 0 || (mCblk = static_cast&lt;audio_track_cblk_t *&gt;(mCblkMemory-&gt;pointer())) == NULL) &#123; ALOGE("not enough memory for AudioTrack size=%zu", size); client-&gt;heap()-&gt;dump("AudioTrack"); mCblkMemory.clear(); return; &#125; &#125; ...... // construct the shared structure in-place. if (mCblk != NULL) &#123; // 这是C++的placement new（定位创建对象）语法：new(@BUFFER) @CLASS(); // 可以在特定内存位置上构造一个对象 // 这里，在匿名共享内存首地址上构造了一个audio_track_cblk_ 对象 // 这样AudioTrack与AudioFlinger都能访问这个audio_track_cblk_t对象了 new(mCblk) audio_track_cblk_t(); switch (alloc) &#123; ...... case ALLOC_CBLK: // clear all buffers if (buffer == NULL) &#123; // 数据传输模式为MODE_STREAM时，数据buffer的分配 // 数据buffer的首地址紧靠控制块（audio_track_cblk_t）之后 mBuffer = (char*)mCblk + sizeof(audio_track_cblk_t); memset(mBuffer, 0, bufferSize); &#125; else &#123; // 数据传输模式为MODE_STATIC时，直接指向sharedBuffer // sharedBuffer是应用进程分配的匿名共享内存，应用进程已经一次性把数据 // 写到sharedBuffer来了，AudioFlinger可以直接从这里读取 mBuffer = buffer; &#125; break; ...... &#125; &#125;&#125;// system/meida/audio/include/system/audio.hstatic inline size_t audio_bytes_per_sample(audio_format_t format)&#123; size_t size = 0; switch (format) &#123; case AUDIO_FORMAT_PCM_32_BIT: case AUDIO_FORMAT_PCM_8_24_BIT: size = sizeof(int32_t); break; case AUDIO_FORMAT_PCM_24_BIT_PACKED: size = sizeof(uint8_t) * 3; break; case AUDIO_FORMAT_PCM_16_BIT: case AUDIO_FORMAT_IEC61937: size = sizeof(int16_t); break; case AUDIO_FORMAT_PCM_8_BIT: size = sizeof(uint8_t); break; case AUDIO_FORMAT_PCM_FLOAT: size = sizeof(float); break; default: break; &#125; return size;&#125; Track从TrackBase继承而来，所以会先执行TrackBase构造函数，mFrameSize会根据format和channel count计算而得，比如双声道，forma为AUDIO_FORMAT_PCM_16_BIT，则mFrameSize为4Byte。计算共享内存大小，其大小为控制块大小加上真正数据大小。size = sizeof(audio_track_cblk_t) + framecount * mFrameSize然后会创建匿名共享内存并且映射到当前进程mCblkMemory = client-&gt;heap()-&gt;allocate(size);然后使用C++的placement new（定位创建对象）在匿名共享内存上创建共享内存控制块。最后将mBuffer清空，创建AudioTrackServerProxy，后续一次操作共享内存读取数据。123456789101112131415161718192021222324252627282930313233343536sp&lt;AudioFlinger::Client&gt; AudioFlinger::registerPid(pid_t pid)&#123; Mutex::Autolock _cl(mClientLock); // If pid is already in the mClients wp&lt;&gt; map, then use that entry // (for which promote() is always != 0), otherwise create a new entry and Client. sp&lt;Client&gt; client = mClients.valueFor(pid).promote(); if (client == 0) &#123; client = new Client(this, pid); mClients.add(pid, client); &#125; return client;&#125;// Max shared memory size for audio tracks and audio records per client processstatic const size_t kClientSharedHeapSizeBytes = 1024*1024;// Shared memory size multiplier for non low ram devicesstatic const size_t kClientSharedHeapSizeMultiplier = 4;AudioFlinger::Client::Client(const sp&lt;AudioFlinger&gt;&amp; audioFlinger, pid_t pid) : RefBase(), mAudioFlinger(audioFlinger), mPid(pid)&#123; size_t heapSize = property_get_int32("ro.af.client_heap_size_kbyte", 0); heapSize *= 1024; if (!heapSize) &#123; heapSize = kClientSharedHeapSizeBytes; // Increase heap size on non low ram devices to limit risk of reconnection // failure for invalidated tracks // 目前一般的设备都不是RAM比较小的设备，所以一般默认申请4M内存 if (!audioFlinger-&gt;isLowRamDevice()) &#123; heapSize *= kClientSharedHeapSizeMultiplier; &#125; &#125; mMemoryDealer = new MemoryDealer(heapSize, "AudioFlinger::Client");&#125; 创建共享内存使用了Client类，client-&gt;heap()-&gt;allocate(size)，创建Client即创建内存管理接口MemoryDealer，即MemoryHeapBase。下一节分析匿名共享内存的C++接口，再具体分析匿名共享内存的管理。 匿名共享内存C++接口MemoryHeapBaseMemoryHeapBase类的对象可以作为Binder对象在进程间传输，作为一个Binder对象，就有Server端对象和Client端引用的概念。下面先看Server端的实现。共同基类RefBase，主要用于智能指针的使用。在ProcessState中打开Binder驱动，IPCThreadState通过ProcessState和Binder驱动交互，当有服务请求时，会先调用IPCThreadState的transact函数，进而调用BBinder的transact函数，根据继承关系会调动BnMemoryHeap的onTransact函数，根据code决定调用具体实现函数，从而进入MemoryHeapBase。这就是一次通信在服务端的运行流程。 MemoryHeapBase要创建共享内存，接下来看看其中一个构造函数具体实现。12345678910111213141516171819202122// frameworks/native/libs/binder/MemoryHeapBase.cppMemoryHeapBase::MemoryHeapBase(size_t size, uint32_t flags, char const * name) : mFD(-1), mSize(0), mBase(MAP_FAILED), mFlags(flags), mDevice(0), mNeedUnmap(false), mOffset(0)&#123; // 获得系统中一页大小的内存 const size_t pagesize = getpagesize(); // 内存页对齐 size = ((size + pagesize-1) &amp; ~(pagesize-1)); // 创建一块匿名共享内存 int fd = ashmem_create_region(name == NULL ? "MemoryHeapBase" : name, size); ALOGE_IF(fd&lt;0, "error creating ashmem region: %s", strerror(errno)); if (fd &gt;= 0) &#123; // 将创建的匿名共享内存映射到当前进程地址空间中 if (mapfd(fd, size) == NO_ERROR) &#123; // 如果地址映射成功，修改匿名共享内存的访问属性 if (flags &amp; READ_ONLY) &#123; ashmem_set_prot_region(fd, PROT_READ); &#125; &#125; &#125;&#125; 在以上构造函数中根据参数，利用匿名共享内存提供的C语言接口创建一块匿名共享内存，并映射到当前进程的虚拟地址空间，参数size是指定匿名共享内存的大小，flags指定匿名共享内存的访问属性，name指定匿名共享内存的名称，如果没有指定名称，默认命名为MemoryHeapBase。123456789101112131415161718192021222324252627282930313233// frameworks/native/libs/binder/MemoryHeapBase.cppstatus_t MemoryHeapBase::mapfd(int fd, size_t size, uint32_t offset)&#123; if (size == 0) &#123; // try to figure out the size automatically struct stat sb; if (fstat(fd, &amp;sb) == 0) size = sb.st_size; // if it didn't work, let mmap() fail. &#125; if ((mFlags &amp; DONT_MAP_LOCALLY) == 0) &#123; // 通过mmap系统调用进入内核空间的匿名共享内存驱动， // 并调用ashmem_mmap函数将匿名共享内存映射到当前进程 void* base = (uint8_t*)mmap(0, size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, offset); if (base == MAP_FAILED) &#123; ALOGE("mmap(fd=%d, size=%u) failed (%s)", fd, uint32_t(size), strerror(errno)); close(fd); return -errno; &#125; //ALOGD("mmap(fd=%d, base=%p, size=%lu)", fd, base, size); mBase = base; mNeedUnmap = true; &#125; else &#123; mBase = 0; // not MAP_FAILED mNeedUnmap = false; &#125; mFD = fd; mSize = size; mOffset = offset; return NO_ERROR;&#125; mmap函数的第一个参数0表示由内核来决定这个匿名共享内存文件在进程地址空间的起始位置，第二个参数size表示要映射的匿名共享内文件的大小，第三个参数PROT_READ|PROT_WRITE表示这个匿名共享内存是可读写的，第四个参数fd指定要映射的匿名共享内存的文件描述符，第五个参数offset表示要从这个文件的哪个偏移位置开始映射。调用了这个函数之后，最后会进入到内核空间的ashmem驱动程序模块中去执行ashmem_mmap函数，调用mmap函数返回之后，就得这块匿名共享内存在本进程地址空间中的起始访问地址了，将这个地址保存在成员变量mBase中，最后，还将这个匿名共享内存的文件描述符和以及大小分别保存在成员变量mFD和mSize，并提供了相应接口函数来访问这些变量值。通过构造MemoryHeapBase对象就可以创建一块匿名共享内存，或者映射一块已经创建的匿名共享内存到当前进程的地址空间。 接下来我们再来看一下MemoryHeapBase在Client端实现。在和匿名共享内存操作相关的类中，BpMemoryHeap类是前面分析的MemoryHeapBase类在Client端进程的远接接口类，当Client端进程从Service Manager或者其它途径获得了一个MemoryHeapBase对象的引用之后，就会在本地创建一个BpMemoryHeap对象来代表这个引用。BpMemoryHeap类同样是要实现IMemoryHeap接口，Client端将调用传递给BpMemoryHeap，BpMemoryHeap的基类BpRefBase有一个mRemote对象，指向BpBinder,所以通过调用BpBinder的transact函数进而调用IPCThreadState，将请求传递给Server端。 BpMemoryHeap的声明如下：12345678910111213141516171819202122232425262728293031323334353637// frameworks/native/libs/binder/IMemory.cppclass BpMemoryHeap : public BpInterface&lt;IMemoryHeap&gt;&#123;public: explicit BpMemoryHeap(const sp&lt;IBinder&gt;&amp; impl); virtual ~BpMemoryHeap(); virtual int getHeapID() const; virtual void* getBase() const; virtual size_t getSize() const; virtual uint32_t getFlags() const; virtual uint32_t getOffset() const;private: friend class IMemory; friend class HeapCache; // for debugging in this module static inline sp&lt;IMemoryHeap&gt; find_heap(const sp&lt;IBinder&gt;&amp; binder) &#123; return gHeapCache-&gt;find_heap(binder); &#125; static inline void free_heap(const sp&lt;IBinder&gt;&amp; binder) &#123; gHeapCache-&gt;free_heap(binder); &#125; static inline sp&lt;IMemoryHeap&gt; get_heap(const sp&lt;IBinder&gt;&amp; binder) &#123; return gHeapCache-&gt;get_heap(binder); &#125; static inline void dump_heaps() &#123; gHeapCache-&gt;dump_heaps(); &#125; void assertMapped() const; void assertReallyMapped() const; mutable std::atomic&lt;int32_t&gt; mHeapId; mutable void* mBase; mutable size_t mSize; mutable uint32_t mFlags; mutable uint32_t mOffset; mutable bool mRealHeap; mutable Mutex mLock;&#125;; 先来看构造函数BpMemoryHeap的实现：12345BpMemoryHeap::BpMemoryHeap(const sp&lt;IBinder&gt;&amp; impl) : BpInterface&lt;IMemoryHeap&gt;(impl), mHeapId(-1), mBase(MAP_FAILED), mSize(0), mFlags(0), mOffset(0), mRealHeap(false)&#123;&#125; 它的实现很简单，只是初始化一下各个成员变量，例如，表示匿名共享内存文件描述符的mHeapId值初化为-1、表示匿名内共享内存基地址的mBase值初始化为MAP_FAILED以及表示匿名共享内存大小的mSize初始为为0，它们都表示在Client端进程中，这个匿名共享内存还未准备就绪，要等到第一次使用时才会去创建。这里还需要注意的一点，参数impl指向的是一个BpBinder对象，它里面包含了一个指向Server端Binder对象，即MemoryHeapBase对象的引用。 以获取共享内存基地址为例，看看Client端如何获取服务端创建的共享内存。123456789101112131415161718192021222324252627282930313233343536// frameworks/native/libs/binder/IMemory.cppvoid* BpMemoryHeap::getBase() const &#123; assertMapped(); return mBase;&#125;void BpMemoryHeap::assertMapped() const&#123; int32_t heapId = mHeapId.load(memory_order_acquire); // 如果还没有请求服务创建匿名共享内存 if (heapId == -1) &#123; // 将当前BpMemoryHeap对象转换为IBinder对象 sp&lt;IBinder&gt; binder(IInterface::asBinder(const_cast&lt;BpMemoryHeap*&gt;(this))); // 从成员变量gHeapCache中查找对应的BpMemoryHeap对象 sp&lt;BpMemoryHeap&gt; heap(static_cast&lt;BpMemoryHeap*&gt;(find_heap(binder).get())); // 向服务端请求获取匿名共享内存信息 heap-&gt;assertReallyMapped(); // 判断该匿名共享内存是否映射成功 if (heap-&gt;mBase != MAP_FAILED) &#123; Mutex::Autolock _l(mLock); // 保存服务端返回回来的匿名共享内存信息 if (mHeapId.load(memory_order_relaxed) == -1) &#123; mBase = heap-&gt;mBase; mSize = heap-&gt;mSize; mOffset = heap-&gt;mOffset; int fd = fcntl(heap-&gt;mHeapId.load(memory_order_relaxed), F_DUPFD_CLOEXEC, 0); ALOGE_IF(fd==-1, "cannot dup fd=%d", heap-&gt;mHeapId.load(memory_order_relaxed)); mHeapId.store(fd, memory_order_release); &#125; &#125; else &#123; // something went wrong free_heap(binder); &#125; &#125;&#125; mHeapId等于-1，表示匿名共享内存还为准备就绪，因此请求服务端MemoryHeapBase创建匿名共享内存，否则该函数不作任何处理。只有第一次使用匿名共享时才会请求服务端创建匿名共享内存。由于在客户端进程中使用同一个BpBinder代理对象可以创建多个与匿名共享内存业务相关的BpMemoryHeap对象，因此定义了类型为HeapCache的全局变量gHeapCache用来保存创建的所有BpMemoryHeap对象，assertMapped函数首先将当前BpMemoryHeap对象强制转换为IBinder类型对象，然后调用find_heap()函数从全局变量gHeapCache中查找出对应的BpMemoryHeap对象，并调用assertReallyMapped()函数向服务进程的BnemoryHeap请求创建匿名共享内存。12345678910111213141516171819202122232425262728293031323334353637383940414243444546// frameworks/native/libs/binder/IMemory.cppvoid BpMemoryHeap::assertReallyMapped() const&#123; int32_t heapId = mHeapId.load(memory_order_acquire); // 再次判断是否已经请求创建过匿名共享内存 if (heapId == -1) &#123; // remote call without mLock held, worse case scenario, we end up // calling transact() from multiple threads, but that's not a problem, // only mmap below must be in the critical section. Parcel data, reply; data.writeInterfaceToken(IMemoryHeap::getInterfaceDescriptor()); // 向服务端BnMemoryHeap发起请求 status_t err = remote()-&gt;transact(HEAP_ID, data, &amp;reply); int parcel_fd = reply.readFileDescriptor(); ssize_t size = reply.readInt32(); uint32_t flags = reply.readInt32(); uint32_t offset = reply.readInt32(); ALOGE_IF(err, "binder=%p transaction failed fd=%d, size=%zd, err=%d (%s)", IInterface::asBinder(this).get(), parcel_fd, size, err, strerror(-err)); Mutex::Autolock _l(mLock); if (mHeapId.load(memory_order_relaxed) == -1) &#123; int fd = fcntl(parcel_fd, F_DUPFD_CLOEXEC, 0); ALOGE_IF(fd==-1, "cannot dup fd=%d, size=%zd, err=%d (%s)", parcel_fd, size, err, strerror(errno)); int access = PROT_READ; if (!(flags &amp; READ_ONLY)) &#123; access |= PROT_WRITE; &#125; mRealHeap = true; // 将服务进程创建的匿名共享内存映射到当前客户进程的地址空间中 mBase = mmap(0, size, access, MAP_SHARED, fd, offset); if (mBase == MAP_FAILED) &#123; ALOGE("cannot map BpMemoryHeap (binder=%p), size=%zd, fd=%d (%s)", IInterface::asBinder(this).get(), size, fd, strerror(errno)); close(fd); &#125; else &#123; // 映射成功后，将匿名共享内存信息保存到BpMemoryHeap的成员变量中，供其他接口函数访问 mSize = size; mFlags = flags; mOffset = offset; mHeapId.store(fd, memory_order_release); &#125; &#125; &#125;&#125; 该函数首先通过Binder通信方式向服务进程请求创建匿名共享内存，当服务端BnMemoryHeap对象创建完匿名共享内存后，并将共享内存信息返回到客户进程后，客户进程通过系统调用mmap函数将匿名共享内存映射到当前进程的地址空间，这样客户进程就可以访问服务进程创建的匿名共享内存了。当了解Binder通信机制，就知道BpMemoryHeap对象通过transact函数向服务端发起请求后，服务端的BnMemoryHeap的onTransact函数会被调用。 在上面获取共享内存的过程中，有一个类型为HeapCache的全局变量gHeapCache。12345678910111213141516171819202122232425// frameworks/native/libs/binder/IMemory.cppclass HeapCache : public IBinder::DeathRecipient&#123;public: HeapCache(); virtual ~HeapCache(); virtual void binderDied(const wp&lt;IBinder&gt;&amp; who); sp&lt;IMemoryHeap&gt; find_heap(const sp&lt;IBinder&gt;&amp; binder); void free_heap(const sp&lt;IBinder&gt;&amp; binder); sp&lt;IMemoryHeap&gt; get_heap(const sp&lt;IBinder&gt;&amp; binder); void dump_heaps();private: // For IMemory.cpp struct heap_info_t &#123; sp&lt;IMemoryHeap&gt; heap; int32_t count; // Note that this cannot be meaningfully copied. &#125;; void free_heap(const wp&lt;IBinder&gt;&amp; binder); Mutex mHeapCacheLock; // Protects entire vector below. KeyedVector&lt; wp&lt;IBinder&gt;, heap_info_t &gt; mHeapCache; // We do not use the copy-on-write capabilities of KeyedVector. // TODO: Reimplemement based on standard C++ container?&#125;;static sp&lt;HeapCache&gt; gHeapCache = new HeapCache(); 它里面定义了一个成员变量mHeapCache，用来维护本进程中的所有BpMemoryHeap对象，同时还提供了find_heap和get_heap函数来查找内部所维护的BpMemoryHeap对象的功能。函数find_heap和get_heap的区别是，在find_heap函数中，如果在mHeapCache找不到相应的BpMemoryHeap对象，就会把这个BpMemoryHeap对象加入到mHeapCache中去，而在get_heap函数中，则不会自动把这个BpMemoryHeap对象加入到mHeapCache中去。 这里，我们主要看一下find_heap函数的实现：1234567891011121314151617181920212223242526// frameworks/native/libs/binder/IMemory.cppsp&lt;IMemoryHeap&gt; HeapCache::find_heap(const sp&lt;IBinder&gt;&amp; binder)&#123; Mutex::Autolock _l(mHeapCacheLock); ssize_t i = mHeapCache.indexOfKey(binder); if (i&gt;=0) &#123; heap_info_t&amp; info = mHeapCache.editValueAt(i); ALOGD_IF(VERBOSE, "found binder=%p, heap=%p, size=%zu, fd=%d, count=%d", binder.get(), info.heap.get(), static_cast&lt;BpMemoryHeap*&gt;(info.heap.get())-&gt;mSize, static_cast&lt;BpMemoryHeap*&gt;(info.heap.get()) -&gt;mHeapId.load(memory_order_relaxed), info.count); ++info.count; return info.heap; &#125; else &#123; heap_info_t info; info.heap = interface_cast&lt;IMemoryHeap&gt;(binder); info.count = 1; //ALOGD("adding binder=%p, heap=%p, count=%d", // binder.get(), info.heap.get(), info.count); mHeapCache.add(binder, info); return info.heap; &#125;&#125; 这个函数很简单，首先它以传进来的参数binder为关键字，在mHeapCache中查找，看看是否有对应的heap_info对象info存在，如果有的话，就增加它的引用计数info.count值，表示这个BpBinder对象多了一个使用者；如果没有的话，那么就需要创建一个heap_info对象info，并且将它加放到mHeapCache中去了。 MemoryBaseMemoryBase接口是建立在MemoryHeapBase接口的基础上的，它们都可以作为一个Binder对象来在进程间进行数据共享，它们的关系如下所示：MemoryBase类包含了一个成员变量mHeap，它的类型的IMemoryHeap，MemoryBase类所代表的匿名共享内存就是通过这个成员变量来实现的。 MemoryBase类在Server端的实现与MemoryHeapBase类在Server端的实现是类似的，这里只要把IMemory类换成IMemoryHeap类、把BnMemory类换成BnMemoryHeap类以及MemoryBase类换成MemoryHeapBase类就变成是MemoryHeapBase类在Server端的实现了，因此，我们这里只简单分析IMemory类和MemoryBase类的实现。1234567891011class IMemory : public IInterface&#123;public: DECLARE_META_INTERFACE(Memory) virtual sp&lt;IMemoryHeap&gt; getMemory(ssize_t* offset=0, size_t* size=0) const = 0; // helpers void* fastPointer(const sp&lt;IBinder&gt;&amp; heap, ssize_t offset) const; void* pointer() const; size_t size() const; ssize_t offset() const;&#125;; 成员函数getMemory用来获取内部的MemoryHeapBase对象的IMemoryHeap接口；成员函数pointer()用来获取内部所维护的匿名共享内存的基地址；成员函数size()用来获取内部所维护的匿名共享内存的大小；成员函数offset()用来获取内部所维护的这部分匿名共享内存在整个匿名共享内存中的偏移量。 IMemory类本身实现了pointer、size和offset三个成员函数，因此，它的子类，即MemoryBase类，只需要实现getMemory成员函数就可以了。IMemory类的实现定义在frameworks/native/libs/binder/IMemory.cpp文件中：1234567891011121314151617181920212223242526void* IMemory::fastPointer(const sp&lt;IBinder&gt;&amp; binder, ssize_t offset) const&#123; sp&lt;IMemoryHeap&gt; realHeap = BpMemoryHeap::get_heap(binder); void* const base = realHeap-&gt;base(); if (base == MAP_FAILED) return 0; return static_cast&lt;char*&gt;(base) + offset;&#125;void* IMemory::pointer() const &#123; ssize_t offset; sp&lt;IMemoryHeap&gt; heap = getMemory(&amp;offset); void* const base = heap!=0 ? heap-&gt;base() : MAP_FAILED; if (base == MAP_FAILED) return 0; return static_cast&lt;char*&gt;(base) + offset;&#125;size_t IMemory::size() const &#123; size_t size; getMemory(NULL, &amp;size); return size;&#125;ssize_t IMemory::offset() const &#123; ssize_t offset; getMemory(&amp;offset); return offset;&#125; 当客户端的BpMemory向服务端MemoryBase发起RPC请求后，服务端的BnMemory对象的onTransact函数被调用12345678910111213141516171819202122status_t BnMemory::onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; switch(code) &#123; case GET_MEMORY: &#123; // 根据客户端发送过来的接口描述进行检查确认 CHECK_INTERFACE(IMemory, data, reply); ssize_t offset; size_t size; // 调用服务端的getMemory函数获取匿名共享内存对象MemoryHeapBase // 及匿名共享内存大小，偏移，并返回给客户端 reply-&gt;writeStrongBinder( IInterface::asBinder(getMemory(&amp;offset, &amp;size)) ); // 将偏移量返回给客户端 reply-&gt;writeInt32(offset); // 将匿名共享内存大小返回给客户端 reply-&gt;writeInt32(size); return NO_ERROR; &#125; break; default: return BBinder::onTransact(code, data, reply, flags); &#125;&#125; 服务端的getMemory函数由BnMemory的子类MemoryBase实现。123456789101112sp&lt;IMemoryHeap&gt; MemoryBase::getMemory(ssize_t* offset, size_t* size) const&#123; if (offset) *offset = mOffset; if (size) *size = mSize; return mHeap;&#125;MemoryBase::MemoryBase(const sp&lt;IMemoryHeap&gt;&amp; heap, ssize_t offset, size_t size) : mSize(size), mOffset(offset), mHeap(heap)&#123;&#125; 在它的构造函数中，接受三个参数，参数heap指向的是一个MemoryHeapBase对象，真正的匿名共享内存就是由它来维护的，参数offset表示这个MemoryBase对象所要维护的这部分匿名共享内存在整个匿名共享内存块中的起始位置，参数size表示这个MemoryBase对象所要维护的这部分匿名共享内存的大小。 成员函数getMemory的实现很简单，只是简单地返回内部的MemoryHeapBase对象的IMemoryHeap接口，如果传进来的参数offset和size不为NULL，还会把其内部维护的这部分匿名共享内存在整个匿名共享内存块中的偏移位置以及这部分匿名共享内存的大小返回给调用者。 这里可以看出，MemoryBase在Server端的实现只是简单地封装了MemoryHeapBase的实现。 MemoryBase类在Client端的实现与MemoryHeapBase类在Client端的实现是类似的，这里只要把IMemory类换成IMemoryHeap类以及把BpMemory类换成BpMemoryHeap类就变成是MemoryHeapBase类在Client端的实现了，因此，我们这里只简单分析BpMemory类的实现，前面已经分析过IMemory类的实现了。1234567891011class BpMemory : public BpInterface&lt;IMemory&gt;&#123;public: explicit BpMemory(const sp&lt;IBinder&gt;&amp; impl); virtual ~BpMemory(); virtual sp&lt;IMemoryHeap&gt; getMemory(ssize_t* offset=0, size_t* size=0) const;private: mutable sp&lt;IMemoryHeap&gt; mHeap; mutable ssize_t mOffset; mutable size_t mSize;&#125;; 下面就看一下BpMemory类的成员函数getMemory的实现：12345678910111213141516171819202122232425262728293031323334353637383940sp&lt;IMemoryHeap&gt; BpMemory::getMemory(ssize_t* offset, size_t* size) const&#123; // 指向的匿名共享内存MemoryHeapBase为空 if (mHeap == 0) &#123; Parcel data, reply; data.writeInterfaceToken(IMemory::getInterfaceDescriptor()); // 向服务端MemoryBase发起RPC请求 if (remote()-&gt;transact(GET_MEMORY, data, &amp;reply) == NO_ERROR) &#123; // 读取服务端返回来的结果 // 读取匿名共享内存MemoryHeapBase的IBinder对象 sp&lt;IBinder&gt; heap = reply.readStrongBinder(); ssize_t o = reply.readInt32(); // 读取匿名共享内存中的偏移量 size_t s = reply.readInt32(); // 读取匿名共享内存的大小 // 如果服务端返回来的用于描述整块匿名共享内存的MemoryHeapBase不为空 if (heap != 0) &#123; mHeap = interface_cast&lt;IMemoryHeap&gt;(heap); if (mHeap != 0) &#123; size_t heapSize = mHeap-&gt;getSize(); // 将匿名共享内存的偏移和大小保存到成员变量中 if (s &lt;= heapSize &amp;&amp; o &gt;= 0 &amp;&amp; (static_cast&lt;size_t&gt;(o) &lt;= heapSize - s)) &#123; mOffset = o; mSize = s; &#125; else &#123; // Hm. android_errorWriteWithInfoLog(0x534e4554, "26877992", -1, NULL, 0); mOffset = 0; mSize = 0; &#125; &#125; &#125; &#125; &#125; // 将成员变量赋值给传进来的参数，从而修改参数值 if (offset) *offset = mOffset; if (size) *size = mSize; return (mSize &gt; 0) ? mHeap : 0;&#125; 如果成员变量mHeap的值为NULL，就表示这个BpMemory对象尚未建立好匿名共享内存，于是，就会通过一个Binder进程间调用去Server端请求匿名共享内存信息，在这些信息中，最重要的就是这个Server端的MemoryHeapBase对象的引用heap了，通过这个引用可以在Client端进程中创建一个BpMemoryHeap远程接口，最后将这个BpMemoryHeap远程接口保存在成员变量mHeap中，同时，从Server端获得的信息还包括这块匿名共享内存在整个匿名共享内存中的偏移位置以及大小。这样，这个BpMemory对象中的匿名共享内存就准备就绪了。 MemoryDealer在AudioFlinger创建Client过程中，使用MemoryDealer申请共享内存,其实MemoryDealer工具类就是对MemoryHeapBase和MemoryBase的封装。1mMemoryDealer = new MemoryDealer(heapSize, "AudioFlinger::Client"); 下面看看MemoryDealer的构造函数，这里会创建MemoryHeapBase，即分配4M共享内存，还会创建与一个简单分配器，后续每次通信需要的具体内存由SimpleBestFitAllocator从4M内存上分配。在SimpleBestFitAllocator中维护一个双向链表来管理这4M的内存。123456789101112131415MemoryDealer::MemoryDealer(size_t size, const char* name, uint32_t flags) : mHeap(new MemoryHeapBase(size, flags, name)), mAllocator(new SimpleBestFitAllocator(size))&#123; &#125;// align all the memory blocks on a cache-line boundaryconst int SimpleBestFitAllocator::kMemoryAlign = 32;SimpleBestFitAllocator::SimpleBestFitAllocator(size_t size)&#123; size_t pagesize = getpagesize(); mHeapSize = ((size + pagesize-1) &amp; ~(pagesize-1)); chunk_t* node = new chunk_t(0, mHeapSize / kMemoryAlign); mList.insertHead(node);&#125; 前面我们看到最终申请内存通过MemoryDealer的allocate的函数，返回的是IMemory类型。从代码看，会返回Allocation，继承自MemoryBase。分配时，最终调用SimpleBestFitAllocator的alloc函数。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172sp&lt;IMemory&gt; MemoryDealer::allocate(size_t size)&#123; sp&lt;IMemory&gt; memory; const ssize_t offset = allocator()-&gt;allocate(size); if (offset &gt;= 0) &#123; memory = new Allocation(this, heap(), offset, size); &#125; return memory;&#125;SimpleBestFitAllocator* MemoryDealer::allocator() const &#123; return mAllocator;&#125;size_t SimpleBestFitAllocator::allocate(size_t size, uint32_t flags)&#123; Mutex::Autolock _l(mLock); ssize_t offset = alloc(size, flags); return offset;&#125;ssize_t SimpleBestFitAllocator::alloc(size_t size, uint32_t flags)&#123; if (size == 0) &#123; return 0; &#125; size = (size + kMemoryAlign-1) / kMemoryAlign; chunk_t* free_chunk = 0; chunk_t* cur = mList.head(); size_t pagesize = getpagesize(); while (cur) &#123; int extra = 0; if (flags &amp; PAGE_ALIGNED) extra = ( -cur-&gt;start &amp; ((pagesize/kMemoryAlign)-1) ) ; // best fit if (cur-&gt;free &amp;&amp; (cur-&gt;size &gt;= (size+extra))) &#123; if ((!free_chunk) || (cur-&gt;size &lt; free_chunk-&gt;size)) &#123; free_chunk = cur; &#125; if (cur-&gt;size == size) &#123; break; &#125; &#125; cur = cur-&gt;next; &#125; if (free_chunk) &#123; const size_t free_size = free_chunk-&gt;size; free_chunk-&gt;free = 0; free_chunk-&gt;size = size; if (free_size &gt; size) &#123; int extra = 0; if (flags &amp; PAGE_ALIGNED) extra = ( -free_chunk-&gt;start &amp; ((pagesize/kMemoryAlign)-1) ) ; if (extra) &#123; chunk_t* split = new chunk_t(free_chunk-&gt;start, extra); free_chunk-&gt;start += extra; mList.insertBefore(free_chunk, split); &#125; ALOGE_IF((flags&amp;PAGE_ALIGNED) &amp;&amp; ((free_chunk-&gt;start*kMemoryAlign)&amp;(pagesize-1)), "PAGE_ALIGNED requested, but page is not aligned!!!"); const ssize_t tail_free = free_size - (size+extra); if (tail_free &gt; 0) &#123; chunk_t* split = new chunk_t( free_chunk-&gt;start + free_chunk-&gt;size, tail_free); mList.insertAfter(free_chunk, split); &#125; &#125; return (free_chunk-&gt;start)*kMemoryAlign; &#125; return NO_MEMORY;&#125; 链表节点，每次从4M空间挖取一段内存作为一个节点，不用时再释放删除节点，通过这种简单的方式来管理这段内存。12345678910struct chunk_t &#123; chunk_t(size_t start, size_t size) : start(start), size(size), free(1), prev(0), next(0) &#123; &#125; size_t start; // 起始地址 size_t size : 28; // 内存大小 int free : 4; // 是否占用 mutable chunk_t* prev; mutable chunk_t* next; &#125;; AudioTrack数据写入和AudioFlinger数据读取AudioTrack实例构造后，应用程序接着可以写入音频数据了。AudioTrack与AudioFlinger是生产者-消费者的关系： AudioTrack：AudioTrack在共享内存中找到一块可用空间，把用户传入的音频数据写入到这块可用空间上，然后更新写位置（对于AudioFinger来说，意味共享内存上有更多的可读数据了）；如果用户传入的数据量比可用空间要大，那么要把用户传入的数据拆分多次写入到共享中（AudioTrack和AudioFlinger是不同的进程，AudioFlinger同时也在不停地读取数据，所以共享内存可用空间是在不停变化的）。 AudioFlinger：AudioFlinger在共享内存中找到一块可读数据块，把可读数据拷贝到目的缓冲区上，然后更新读位置（对于AudioTrack来说，意味着共享内存上有更多的可用空间了）；如果共享内存上可读数据量比预期的要小，那么要进行多次的读取，才能积累到预期的数据量（AudioTrack和AudioFlinger是不同的进程，AudioTrack同时也在不停地写入数据，所以共享内存可读的数据量是在不停变化的）。 在AudioTrack和AudioFlinger操作共享内存的时使用Proxy来管理。1234567891011121314151617181920212223242526272829303132// frameworks/av/include/private/meida/AudioTrackShared.h// Proxy for shared memory control block, to isolate callers // from needing to know the details. There is exactly one// ClientProxy and one ServerProxy per shared memory control block.// The proxies are located in normal memory, // and are not multi-thread safe within a given side.class Proxy : public RefBase &#123;protected: Proxy(audio_track_cblk_t* cblk, void *buffers, size_t frameCount, size_t frameSize, bool isOut, bool clientInServer); virtual ~Proxy() &#123; &#125;public: size_t frameCount() const &#123; return mFrameCount; &#125;protected: // These refer to shared memory, and are virtual addresses with respect to the // current process. They may have different virtual addresses within the other process. audio_track_cblk_t* const mCblk; // the control block void* const mBuffers; // starting address of buffers const size_t mFrameCount; // not necessarily a power of 2 const size_t mFrameSize; // in bytes const size_t mFrameCountP2; // mFrameCount rounded to power of 2, streaming mode const bool mIsOut; // true for AudioTrack, false for AudioRecord const bool mClientInServer; // true for OutputTrack, // false for AudioTrack &amp; AudioRecord bool mIsShutdown; // latch set to true when // shared memory corruption detected size_t mUnreleased; // unreleased frames remaining // from most recent obtainBuffer&#125;; 如下是Proxy的类图： AudioTrackClientProxy：MODE_STREAM模式下，生产者AudioTrack使用它在共享内存中找到可用空间的位置 AudioTrackServerProxy：MODE_STREAM模式下，消费者AudioFlinger::PlaybackThread使用它在 共享内存中找到可读数据的位置 StaticAudioTrackClientProxy：MODE_STATIC模式下，生产者AudioTrack使用它在共享内存中找到可用空间的位置 StaticAudioTrackServerProxy：MODE_STATIC模式下，消费者AudioFlinger::PlaybackThread 使用它在共享内存中找到可读数据的位置 AudioRecordClientProxy：消费者AudioRecord使用它在共享内存中找到可读数据的位置 AudioTrackServerProxy：生产者AudioFlinger::RecordThread使用它在共享内存中找到可用空间的位置 AudioTrack数据写入在写数据的过程中会用到两个buffer，分别是AudioTrack::Buffer和Proxy::Buffer，它们声明如下。12345678910111213141516171819202122232425262728293031323334// frameworks/av/media/libaudioclient/include/media/AudioTrack.h// AudioTrack::Buffer声明class Buffer&#123;public: // FIXME use m prefix size_t frameCount; // number of sample frames corresponding to size; // on input to obtainBuffer() it is the number of frames desired, // on output from obtainBuffer() it is the number of available // [empty slots for] frames to be filled // on input to releaseBuffer() it is currently ignored size_t size; // input/output in bytes == frameCount * frameSize // on input to obtainBuffer() it is ignored // on output from obtainBuffer() it is the number of available // [empty slots for] bytes to be filled, // which is frameCount * frameSize // on input to releaseBuffer() it is the number of bytes to // release // FIXME This is redundant with respect to frameCount. Consider // removing size and making frameCount the primary field. union &#123; void* raw; short* i16; // signed 16-bit int8_t* i8; // unsigned 8-bit, offset by 0x80 &#125;; // input to obtainBuffer(): unused, output: pointer to buffer&#125;;// frameworks/av/include/private/meida/AudioTrackShared.h// Proxy::Buffer声明struct Buffer &#123; size_t mFrameCount; // number of frames available in this buffer void* mRaw; // pointer to first frame size_t mNonContig; // number of additional non-contiguous frames available&#125;; 当创建好AudioTrack，Client端会执行写数据，则会调用AudioTrack的write函数向硬件写数据，首先会获取可用的共享内存空间，将数据拷贝到这块可用空间，然后更新共享内存写数据的位置。123456789101112131415161718192021222324252627282930313233343536// frameworks/av/media/libaudioclient/AudioTrack.cppssize_t AudioTrack::write(const void* buffer, size_t userSize, bool blocking)&#123; // 条件检查 ...... size_t written = 0; Buffer audioBuffer; //如上声明 while (userSize &gt;= mFrameSize) &#123; // 单帧数据量 frameSize = channelCount * bytesPerSample // 对于双声道，16位采样的音频数据来说，frameSize = 2 * 2 = 4(bytes) // 用户传入的数据帧数 frameCount = userSize / frameSize audioBuffer.frameCount = userSize / mFrameSize; // obtainBuffer() 从共享内存上得到一块可用区间 status_t err = obtainBuffer(&amp;audioBuffer, blocking ? &amp;ClientProxy::kForever : &amp;ClientProxy::kNonBlocking); ...... // toWrite 是共享内存上可用区间的大小，可能比userSize（用户传入数据的大小）要小 // 因此用户传入的数据可能要拆分多次拷贝到共享内存上 // 注意：AudioTrack和AudioFlinger是不同的进程，AudioFlinger同时也在不停地 // 消耗数据，所以共享内存可用区间是在不停变化的 size_t toWrite = audioBuffer.size; memcpy(audioBuffer.i8, buffer, toWrite); // 把用户数据拷贝到共享内存可用区间 buffer = ((const char *) buffer) + toWrite; // 未拷贝数据的位置 userSize -= toWrite; // 未拷贝数据的大小 written += toWrite; // 已拷贝数据的大小 // releaseBuffer() 更新共享内存写位置 // 对于AudioFinger来说，意味共享内存上有更多的可读数据 releaseBuffer(&amp;audioBuffer); &#125; if (written &gt; 0) &#123; mFramesWritten += written / mFrameSize; &#125; return written;&#125; 在AudioTrack内部会先设置进程睡眠时间，然后调用AudioTrackClientProxy的obtainBuffer函数获取Proxy::Buffer，然后将其转换为AudioTrack::Buffer，然后通过参数返回。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// frameworks/av/media/libaudioclient/AudioTrack.cppstatus_t AudioTrack::obtainBuffer(Buffer* audioBuffer, int32_t waitCount, size_t *nonContig)&#123; // 条件检查 ..... const struct timespec *requested; struct timespec timeout; // 通过waitCount的值计算是否需要等待，若要等待则等待多久 ..... return obtainBuffer(audioBuffer, requested, NULL /*elapsed*/, nonContig);&#125;status_t AudioTrack::obtainBuffer(Buffer* audioBuffer, const struct timespec *requested, struct timespec *elapsed, size_t *nonContig)&#123; // previous and new IAudioTrack sequence numbers are used to detect track re-creation uint32_t oldSequence = 0; uint32_t newSequence; Proxy::Buffer buffer; status_t status = NO_ERROR; static const int32_t kMaxTries = 5; // 最多5次尝试获取可用空间 int32_t tryCounter = kMaxTries; do &#123; // obtainBuffer() is called with mutex unlocked, so keep extra references // to these fields to keep them from going away // if another thread re-creates the track during obtainBuffer() sp&lt;AudioTrackClientProxy&gt; proxy; sp&lt;IMemory&gt; iMem; ...... // 对异常状况，stop状态等情况的处理，并获取AudioTrackClientProxy // 调用AudioTrackClientProxy的obtainBuffer函数，由于继承关系， // 会调用ClientProxy的obtainBuffer函数 buffer.mFrameCount = audioBuffer-&gt;frameCount; // FIXME starts the requested timeout and elapsed over from scratch status = proxy-&gt;obtainBuffer(&amp;buffer, requested, elapsed); &#125; while (((status == DEAD_OBJECT) || (status == NOT_ENOUGH_DATA)) &amp;&amp; (tryCounter-- &gt; 0)); 将Proxy::Buffer转为AudioTrack::Buffer audioBuffer-&gt;frameCount = buffer.mFrameCount; audioBuffer-&gt;size = buffer.mFrameCount * mFrameSize; audioBuffer-&gt;raw = buffer.mRaw; if (nonContig != NULL) &#123; *nonContig = buffer.mNonContig; &#125; return status;&#125; 在ClientProxy中，首先设置timeout类型，然后从共享内存读取队头和队尾指针，从而计算已经使用的区域，结合共享内存的大小计算可用空间，最后找到一块合适的空间返回。在其中还会通过系统调用进行进程间的同步控制，看具体情况将进程挂起。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146// frameworks/av/media/libaudioclient/AudioTrackShared.cpp__attribute__((no_sanitize("integer")))status_t ClientProxy::obtainBuffer(Buffer* buffer, const struct timespec *requested, struct timespec *elapsed)&#123; LOG_ALWAYS_FATAL_IF(buffer == NULL || buffer-&gt;mFrameCount == 0); struct timespec total; // total elapsed time spent waiting total.tv_sec = 0; total.tv_nsec = 0; bool measure = elapsed != NULL; // whether to measure total elapsed time spent waiting status_t status; enum &#123; TIMEOUT_ZERO, // requested == NULL || *requested == 0 TIMEOUT_INFINITE, // *requested == infinity TIMEOUT_FINITE, // 0 &lt; *requested &lt; infinity TIMEOUT_CONTINUE, // additional chances after TIMEOUT_FINITE &#125; timeout; ...... // 根据requested设置timeout类型 struct timespec before; bool beforeIsValid = false; audio_track_cblk_t* cblk = mCblk; bool ignoreInitialPendingInterrupt = true; // check for shared memory corruption if (mIsShutdown) &#123; status = NO_INIT; goto end; &#125; for (;;) &#123; ...... // 基本条件检查 int32_t front; int32_t rear; // 注意使用带内存屏障的函数android_atomic_acquire_load // 获取队头和队尾 if (mIsOut) &#123; // 对应AudioTrack // The barrier following the read of mFront is probably redundant. // We're about to perform a conditional branch based on 'filled', // which will force the processor to observe the read of mFront // prior to allowing data writes starting at mRaw. // However, the processor may support speculative execution, // and be unable to undo speculative writes into shared memory. // The barrier will prevent such speculative execution. front = android_atomic_acquire_load(&amp;cblk-&gt;u.mStreaming.mFront); rear = cblk-&gt;u.mStreaming.mRear; &#125; else &#123; // 对应AudioRecord // On the other hand, this barrier is required. rear = android_atomic_acquire_load(&amp;cblk-&gt;u.mStreaming.mRear); front = cblk-&gt;u.mStreaming.mFront; &#125; // write to rear, read from front // 已经使用的区域 ssize_t filled = rear - front; // pipe should not be overfull // 当已经使用的空间大于预先设置的帧数，对于播放来讲出错了， // 而对于录音来说目前处于overrun,写入的速度太快，读取熟读跟不上 if (!(0 &lt;= filled &amp;&amp; (size_t) filled &lt;= mFrameCount)) &#123; if (mIsOut) &#123; ALOGE("Shared memory control block is corrupt (filled=%zd, mFrameCount=%zu); " "shutting down", filled, mFrameCount); mIsShutdown = true; status = NO_INIT; goto end; &#125; // for input, sync up on overrun filled = 0; cblk-&gt;u.mStreaming.mFront = rear; (void) android_atomic_or(CBLK_OVERRUN, &amp;cblk-&gt;mFlags); &#125; // Don't allow filling pipe beyond the user settable size. // The calculation for avail can go negative if the buffer size // is suddenly dropped below the amount already in the buffer. // So use a signed calculation to prevent a numeric overflow abort. // 有符号运算可用空间 ssize_t adjustableSize = (ssize_t) getBufferSizeInFrames(); ssize_t avail = (mIsOut) ? adjustableSize - filled : filled; if (avail &lt; 0) &#123; avail = 0; &#125; else if (avail &gt; 0) &#123; // 'avail' may be non-contiguous, so return only the first contiguous chunk size_t part1; if (mIsOut) &#123; rear &amp;= mFrameCountP2 - 1; part1 = mFrameCountP2 - rear; &#125; else &#123; front &amp;= mFrameCountP2 - 1; part1 = mFrameCountP2 - front; &#125; if (part1 &gt; (size_t)avail) &#123; part1 = avail; &#125; if (part1 &gt; buffer-&gt;mFrameCount) &#123; part1 = buffer-&gt;mFrameCount; &#125; // 赋值Proxy::Buffer buffer-&gt;mFrameCount = part1; buffer-&gt;mRaw = part1 &gt; 0 ? &amp;((char *) mBuffers)[(mIsOut ? rear : front) * mFrameSize] : NULL; buffer-&gt;mNonContig = avail - part1; mUnreleased = part1; status = NO_ERROR; break; &#125; struct timespec remaining; const struct timespec *ts; ...... // 根据timeout类型计算剩余等待的时间ts int32_t old = android_atomic_and(~CBLK_FUTEX_WAKE, &amp;cblk-&gt;mFutex); if (!(old &amp; CBLK_FUTEX_WAKE)) &#123; if (measure &amp;&amp; !beforeIsValid) &#123; clock_gettime(CLOCK_MONOTONIC, &amp;before); beforeIsValid = true; &#125; errno = 0; // 同步控制，down (P)操作,原子性的给cblk-&gt;mFutex同步变量减1， // FUTEX_WAIT，原子性的检查cblk-&gt;mFutex中计数器的值是否为负值, // 如果是则让进程休眠，直到FUTEX_WAKE或者超时(time-out)。也就是 // 把进程挂到cblk-&gt;mFutex相对应的等待队列上去。 (void) syscall(__NR_futex, &amp;cblk-&gt;mFutex, mClientInServer ? FUTEX_WAIT_PRIVATE : FUTEX_WAIT, old &amp; ~CBLK_FUTEX_WAKE, ts); status_t error = errno; // clock_gettime can affect errno // update total elapsed time spent waiting ...... switch (error) &#123; case 0: // normal wakeup by server, or by binderDied() case EWOULDBLOCK: // benign race condition with server case EINTR: // wait was interrupted by signal or other spurious wakeup case ETIMEDOUT: // time-out expired // FIXME these error/non-0 status are being dropped break; default: status = error; ALOGE("%s unexpected error %s", __func__, strerror(status)); goto end; &#125; &#125; &#125;end: ...... // 善后处理 return status;&#125; Client端拷贝完会更新共享内存的写数据指针，这一步通过ClientProxy的releaseBuffer实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445// frameworks/av/media/libaudioclient/AudioTrack.cppvoid AudioTrack::releaseBuffer(const Buffer* audioBuffer)&#123; // FIXME add error checking on mode, by adding an internal version ..... // 条件检查 // 数据写入完毕，将数据从AudioTrack::Buffer转入Proxy::Buffer， // 调用AudioTrackClientProxy的releaseBuffer函数释放buffer控制权 Proxy::Buffer buffer; buffer.mFrameCount = stepCount; buffer.mRaw = audioBuffer-&gt;raw; AutoMutex lock(mLock); mReleased += stepCount; mInUnderrun = false; mProxy-&gt;releaseBuffer(&amp;buffer); // restart track if it was disabled by audioflinger due to previous underrun restartIfDisabled();&#125;// frameworks/av/media/libaudioclient/AudioTrackShared.cpp__attribute__((no_sanitize("integer")))void ClientProxy::releaseBuffer(Buffer* buffer)&#123; LOG_ALWAYS_FATAL_IF(buffer == NULL); size_t stepCount = buffer-&gt;mFrameCount; if (stepCount == 0 || mIsShutdown) &#123; // prevent accidental re-use of buffer buffer-&gt;mFrameCount = 0; buffer-&gt;mRaw = NULL; buffer-&gt;mNonContig = 0; return; &#125; LOG_ALWAYS_FATAL_IF(!(stepCount &lt;= mUnreleased &amp;&amp; mUnreleased &lt;= mFrameCount)); mUnreleased -= stepCount; audio_track_cblk_t* cblk = mCblk; // Both of these barriers are required // 释放buffer控制权，其实是移动队尾（队头），更新写数据位置，注意内存屏障的使用 if (mIsOut) &#123; int32_t rear = cblk-&gt;u.mStreaming.mRear; android_atomic_release_store(stepCount + rear, &amp;cblk-&gt;u.mStreaming.mRear); &#125; else &#123; int32_t front = cblk-&gt;u.mStreaming.mFront; android_atomic_release_store(stepCount + front, &amp;cblk-&gt;u.mStreaming.mFront); &#125;&#125; AudioFlinger数据读取在AudioFlinger端读取数据会使用到AudioBufferProvider::Buffer和Proxy::Buffer。12345678910111213141516171819// frameworks/av/media/libaudioclient/include/media/AudioBufferProvider.hAudioBufferProvider::Buffer声明struct Buffer &#123; Buffer() : raw(NULL), frameCount(0) &#123; &#125; union &#123; void* raw; short* i16; int8_t* i8; &#125;; size_t frameCount;&#125;;// frameworks/av/include/private/meida/AudioTrackShared.h// Proxy::Buffer声明struct Buffer &#123; size_t mFrameCount; // number of frames available in this buffer void* mRaw; // pointer to first frame size_t mNonContig; // number of additional non-contiguous frames available&#125;; 我们以DirectOutputThread/OffloadThread为例说明（MixerThread读数据也是类似的过程，由于MixerThread会有混音过程，所以读取数据会稍微复杂点，是在AudioMixer中进行的，后续有机会分析混音时在分析其读取数据的过程）。1234567891011121314151617181920212223242526272829303132333435363738// frameworks/av/services/audioflinger/Threads.cppvoid AudioFlinger::DirectOutputThread::threadLoop_mix()&#123; // mFrameCount是硬件设备（PCM设备）处理单个数据块的帧数（周期大小） // 上层必须积累了足够多（mFrameCount）的数据，才写入到PCM设备所以 // mFrameCount也就是AudioFlinger预期的数据量 size_t frameCount = mFrameCount; // mSinkBuffer目的缓冲区，threadLoop_write() 会把mSinkBuffer上的数据写到PCM设备 int8_t *curBuf = (int8_t *)mSinkBuffer; // output audio to hardware // FIFO 上可读的数据量可能要比预期的要小，因此可能需要多次读取才能积累足够的数据量 // 注意：AudioTrack和AudioFlinger是不同的进程，AudioTrack同时也在不停地生产数据 // 所以共享内存可读的数据量是在不停变化的 while (frameCount) &#123; AudioBufferProvider::Buffer buffer; buffer.frameCount = frameCount; // getNextBuffer() 从共享内存上获取可读数据块 status_t status = mActiveTrack-&gt;getNextBuffer(&amp;buffer); if (status != NO_ERROR || buffer.raw == NULL) &#123; // no need to pad with 0 for compressed audio if (audio_has_proportional_frames(mFormat)) &#123; memset(curBuf, 0, frameCount * mFrameSize); &#125; break; &#125; // memcpy()把共享内存可读数据拷贝到mSinkBuffer目的缓冲区 memcpy(curBuf, buffer.raw, buffer.frameCount * mFrameSize); frameCount -= buffer.frameCount; curBuf += buffer.frameCount * mFrameSize; // releaseBuffer()更新共享内存读位置 // 对于AudioTrack来说，意味着共享内存上有更多的可用空间 mActiveTrack-&gt;releaseBuffer(&amp;buffer); &#125; mCurrentWriteLength = curBuf - (int8_t *)mSinkBuffer; mSleepTimeUs = 0; mStandbyTimeNs = systemTime() + mStandbyDelayNs; mActiveTrack.clear();&#125; 从上面看到会先通过mActiveTrack的getNextBuffer获取可读数据，即Track类的getNextBuffer函数，然后将数据拷贝到目的mSinkBuffer，然后更新读数据指针位置。12345678910111213141516171819202122232425262728293031323334353637383940// frameworks/av/services/audioflinger/Tracks.cpp// AudioBufferProvider interfacestatus_t AudioFlinger::PlaybackThread::Track::getNextBuffer( AudioBufferProvider::Buffer* buffer)&#123; ServerProxy::Buffer buf; size_t desiredFrames = buffer-&gt;frameCount; buf.mFrameCount = desiredFrames; // 调用AudioTrackServerProxy的obtainBuffer函数，由于继承关系， // 会调用ServerProxy的obtainBuffer函数 status_t status = mServerProxy-&gt;obtainBuffer(&amp;buf); // Proxy::Buffer转化为AudioBufferProvider::Buffer buffer-&gt;frameCount = buf.mFrameCount; buffer-&gt;raw = buf.mRaw; // 是否处于underrun状态 if (buf.mFrameCount == 0 &amp;&amp; !isStopping() &amp;&amp; !isStopped() &amp;&amp; !isPaused()) &#123; ALOGV("underrun, framesReady(%zu) &lt; framesDesired(%zd), state: %d", buf.mFrameCount, desiredFrames, mState); mAudioTrackServerProxy-&gt;tallyUnderrunFrames(desiredFrames); &#125; else &#123; mAudioTrackServerProxy-&gt;tallyUnderrunFrames(0); &#125; return status;&#125;// AudioBufferProvider interface// getNextBuffer() = 0;// This implementation of releaseBuffer() is used by Track and RecordTrackvoid AudioFlinger::ThreadBase::TrackBase::releaseBuffer(AudioBufferProvider::Buffer* buffer)&#123; // AudioBufferProvider::Buffer转化为Proxy::Buffer ServerProxy::Buffer buf; buf.mFrameCount = buffer-&gt;frameCount; buf.mRaw = buffer-&gt;raw; buffer-&gt;frameCount = 0; buffer-&gt;raw = NULL; // 调用AudioTrackServerProxy的releaseBuffer函数，由于继承关系， // 会调用ServerProxy的releaseBuffer函数 mServerProxy-&gt;releaseBuffer(&amp;buf);&#125; 获取可读数据及更新读数据指针位置最终会通过ServerProxy实现，同样先获取队头指针和队尾指针，然后计算可读数据，将可读数据转给传入的buffer。拷贝完数据，然后更新读数据指针，然后同步通知Client端，唤醒挂起的进程。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122// frameworks/av/media/libaudioclient/AudioTrackShared.cpp__attribute__((no_sanitize("integer")))status_t ServerProxy::obtainBuffer(Buffer* buffer, bool ackFlush)&#123; LOG_ALWAYS_FATAL_IF(buffer == NULL || buffer-&gt;mFrameCount == 0, "%s: null or zero frame buffer, buffer:%p", __func__, buffer); if (mIsShutdown) &#123; goto no_init; &#125; &#123; audio_track_cblk_t* cblk = mCblk; // compute number of frames available to write (AudioTrack) or read (AudioRecord), // or use previous cached value from framesReady(), with added barrier if it omits. int32_t front; int32_t rear; // See notes on barriers at ClientProxy::obtainBuffer() // 注意使用带内存屏障的函数android_atomic_acquire_load // 获取队头和队尾 if (mIsOut) &#123; flushBufferIfNeeded(); // might modify mFront rear = android_atomic_acquire_load(&amp;cblk-&gt;u.mStreaming.mRear); front = cblk-&gt;u.mStreaming.mFront; &#125; else &#123; front = android_atomic_acquire_load(&amp;cblk-&gt;u.mStreaming.mFront); rear = cblk-&gt;u.mStreaming.mRear; &#125; ssize_t filled = rear - front; // pipe should not already be overfull if (!(0 &lt;= filled &amp;&amp; (size_t) filled &lt;= mFrameCount)) &#123; ALOGE("Shared memory control block is corrupt (filled=%zd, mFrameCount=%zu); shutting down", filled, mFrameCount); mIsShutdown = true; &#125; if (mIsShutdown) &#123; goto no_init; &#125; // don't allow filling pipe beyond the nominal size size_t availToServer; if (mIsOut) &#123; availToServer = filled; mAvailToClient = mFrameCount - filled; &#125; else &#123; availToServer = mFrameCount - filled; mAvailToClient = filled; &#125; // 'availToServer' may be non-contiguous, so return only the first contiguous chunk size_t part1; if (mIsOut) &#123; front &amp;= mFrameCountP2 - 1; part1 = mFrameCountP2 - front; &#125; else &#123; rear &amp;= mFrameCountP2 - 1; part1 = mFrameCountP2 - rear; &#125; if (part1 &gt; availToServer) &#123; part1 = availToServer; &#125; size_t ask = buffer-&gt;mFrameCount; if (part1 &gt; ask) &#123; part1 = ask; &#125; // is assignment redundant in some cases? buffer-&gt;mFrameCount = part1; buffer-&gt;mRaw = part1 &gt; 0 ? &amp;((char *) mBuffers)[(mIsOut ? front : rear) * mFrameSize] : NULL; buffer-&gt;mNonContig = availToServer - part1; // After flush(), allow releaseBuffer() on a previously obtained buffer; // see "Acknowledge any pending flush()" in audioflinger/Tracks.cpp. if (!ackFlush) &#123; mUnreleased = part1; &#125; return part1 &gt; 0 ? NO_ERROR : WOULD_BLOCK; &#125;no_init: buffer-&gt;mFrameCount = 0; buffer-&gt;mRaw = NULL; buffer-&gt;mNonContig = 0; mUnreleased = 0; return NO_INIT;&#125;__attribute__((no_sanitize("integer")))void ServerProxy::releaseBuffer(Buffer* buffer)&#123; ..... // 条件检查 mUnreleased -= stepCount; audio_track_cblk_t* cblk = mCblk; // 释放buffer控制权，其实是移动队头（队尾），更新读数据位置，注意内存屏障的使用 if (mIsOut) &#123; int32_t front = cblk-&gt;u.mStreaming.mFront; android_atomic_release_store(stepCount + front, &amp;cblk-&gt;u.mStreaming.mFront); &#125; else &#123; int32_t rear = cblk-&gt;u.mStreaming.mRear; android_atomic_release_store(stepCount + rear, &amp;cblk-&gt;u.mStreaming.mRear); &#125; cblk-&gt;mServer += stepCount; mReleased += stepCount; size_t half = mFrameCount / 2; if (half == 0) &#123; half = 1; &#125; size_t minimum = (size_t) cblk-&gt;mMinimum; if (minimum == 0) &#123; minimum = mIsOut ? half : 1; &#125; else if (minimum &gt; half) &#123; minimum = half; &#125; // FIXME AudioRecord wakeup needs to be optimized; it currently wakes up client every time if (!mIsOut || (mAvailToClient + stepCount &gt;= minimum)) &#123; int32_t old = android_atomic_or(CBLK_FUTEX_WAKE, &amp;cblk-&gt;mFutex); if (!(old &amp; CBLK_FUTEX_WAKE)) &#123; // 同步控制，up (V)操作,原子性的给cblk-&gt;mFutex同步变量加1， // FUTEX_WAKE，原子性的检查cblk-&gt;mFutex中计数器的值是否为正值, // 如果不是则唤醒一个或者多个等待进程。 (void) syscall(__NR_futex, &amp;cblk-&gt;mFutex, mClientInServer ? FUTEX_WAKE_PRIVATE : FUTEX_WAKE, 1); &#125; &#125; buffer-&gt;mFrameCount = 0; buffer-&gt;mRaw = NULL; buffer-&gt;mNonContig = 0;&#125;]]></content>
      <categories>
        <category>Android Audio</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android AudioManager API: getDevicces原理]]></title>
    <url>%2F2017%2F12%2F16%2FAndroid-AudioManager-API-getDevicces%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[阅读Android开发者文档，我们看到在AudioManager中判断有线耳机和A2DP的是否存在有两组API，分别是isWiredHeadsetOn()，isBluetoothA2dpOn()和getDevices(int)，而isWiredHeadsetOn()在API 14中废弃，isBluetoothA2dpOn()在API 26中废弃，google建议都使用getDevices(int)，而在使用getDevices(int)过程中遇到bug，所以本文就研究一下getDevices(int)的原理。 宏观剖析getDevices首先我们通过一张图，从进程的角度看看getDevices API的原理。我们可以将getDevices大概分为两种情况讨论，第一次调用： Client调用AudioManager的getDevices函数，获取MainLooper，创建Handler。 获取AudioPolicyService的代理，创建AudioPolicyServiceClient，添加回调AudioPortCallback。 将AudioPolicyServiceClien注册到AudioPolicyService，更新cache，获取可用设备list返回。 我们再看第二种情况，在有新设备和手机连接的时候： 此时会进入AudioPolicyService进程中，调用AudioPolicyManager的setDeviceConnectionStateInt函数。 然后调用AudioPolicyClient的onAudioPortListUpdate，通过AudioPolicyService启动一文，我们知道，最终会调用AudioPolicyService的onAudioPortListUpdate 调用所有AudioPolicyServiceClien的AudioPortCallback，获取之前创建的Handler。 发送设备更新消息。 添加消息到消息队列。 Looper取出消息队列中的消息。 处理设备更新消息。 后续该Client调用getDevices重新获取设备list，更新cache。 时序图首先我们看看第一次调用getDevices的过程，在这个过程中，主要由两个工作，一是注册回调通知，二是获取设备列表；后面再次调用就不需要再次注册了，直接获取设备列表即可。下面是当设备变化时的回调通知过程。 微观剖析getDevicesJava层调用在AudioManager中会调用从getDevicesStatic，listAudioDevicePorts到updateAudioPortCache，由于主要的处理都在updateAudioPortCache中，所以我们主要看一下该函数，在其中先初始化AudioPortEventHandler，若sAudioPortGeneration是初始值，则更新sAudioPortsCached，并返回其中的值，否则直接返回sAudioPortsCached中的值。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public AudioDeviceInfo[] getDevices(int flags) &#123; return getDevicesStatic(flags);&#125;public static AudioDeviceInfo[] getDevicesStatic(int flags) &#123; ...... int status = AudioManager.listAudioDevicePorts(ports); ......&#125;public static int listAudioDevicePorts(ArrayList&lt;AudioDevicePort&gt; devices) &#123; ...... int status = updateAudioPortCache(ports, null, null); ......&#125;static int resetAudioPortGeneration() &#123; int generation; synchronized (sAudioPortGeneration) &#123; generation = sAudioPortGeneration; sAudioPortGeneration = AUDIOPORT_GENERATION_INIT; &#125; return generation;&#125;static int updateAudioPortCache(ArrayList&lt;AudioPort&gt; ports, ArrayList&lt;AudioPatch&gt; patches, ArrayList&lt;AudioPort&gt; previousPorts) &#123; sAudioPortEventHandler.init(); synchronized (sAudioPortGeneration) &#123; if (sAudioPortGeneration == AUDIOPORT_GENERATION_INIT) &#123; ...... do &#123; newPorts.clear(); status = AudioSystem.listAudioPorts(newPorts, portGeneration); ...... &#125; while (patchGeneration[0] != portGeneration[0]); ...... sAudioPortGeneration = portGeneration[0]; &#125; if (ports != null) &#123; ports.clear(); ports.addAll(sAudioPortsCached); &#125; ...... &#125; return SUCCESS;&#125; 在updateAudioPortCache会初始化AudioPortEventHandler，下面我们看看具体会做哪些事情。首先会获取当前进程的MainLooper，并以此Looper创建Handler，重写Handler消息处理函数。然后调用native_setup进入JNI层。而handleMessage对设备更新消息的处理是resetAudioPortGeneration，这样就会是的下次调用getDevices会更新sAudioPortsCached，从而得到更新后的设备列表。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class AudioPortEventHandler &#123; void init() &#123; synchronized (this) &#123; if (mHandler != null) &#123; return; &#125; // find the looper for our new event handler Looper looper = Looper.getMainLooper(); if (looper != null) &#123; mHandler = new Handler(looper) &#123; @Override public void handleMessage(Message msg) &#123; ...... if (msg.what == AUDIOPORT_EVENT_PORT_LIST_UPDATED || msg.what == AUDIOPORT_EVENT_PATCH_LIST_UPDATED || msg.what == AUDIOPORT_EVENT_SERVICE_DIED) &#123; AudioManager.resetAudioPortGeneration(); &#125; if (listeners.isEmpty()) &#123; return; &#125; ArrayList&lt;AudioPort&gt; ports = new ArrayList&lt;AudioPort&gt;(); ArrayList&lt;AudioPatch&gt; patches = new ArrayList&lt;AudioPatch&gt;(); if (msg.what != AUDIOPORT_EVENT_SERVICE_DIED) &#123; int status = AudioManager.updateAudioPortCache(ports, patches, null); if (status != AudioManager.SUCCESS) &#123; return; &#125; &#125; ...... &#125; &#125;; native_setup(new WeakReference&lt;AudioPortEventHandler&gt;(this)); &#125; else &#123; mHandler = null; &#125; &#125; &#125; private native void native_setup(Object module_this); @SuppressWarnings("unused") private static void postEventFromNative(Object module_ref, int what, int arg1, int arg2, Object obj) &#123; AudioPortEventHandler eventHandler = (AudioPortEventHandler)((WeakReference)module_ref).get(); if (eventHandler == null) &#123; return; &#125; if (eventHandler != null) &#123; Handler handler = eventHandler.handler(); if (handler != null) &#123; Message m = handler.obtainMessage(what, arg1, arg2, obj); handler.sendMessage(m); &#125; &#125; &#125;&#125; JNI层调用仔细追代码，发现AudioPortEventHandler的JNI函数定义在android_media_AudioSystem.cpp。且Java实例方法native_setup对应着JNI层android_media_AudioSystem_eventHandlerSetup。同时还会有Java静态方法postEventFromNative和JNI层的gAudioPortEventHandlerMethods.postEventFromNative的对应。1234567891011121314151617181920212223static const char* const kEventHandlerClassPathName = "android/media/AudioPortEventHandler";static const JNINativeMethod gEventHandlerMethods[] = &#123; &#123;"native_setup", "(Ljava/lang/Object;)V", (void *)android_media_AudioSystem_eventHandlerSetup&#125;, &#123;"native_finalize", "()V", (void *)android_media_AudioSystem_eventHandlerFinalize&#125;,&#125;;int register_android_media_AudioSystem(JNIEnv *env) &#123; ...... jclass eventHandlerClass = FindClassOrDie(env, kEventHandlerClassPathName); gAudioPortEventHandlerMethods.postEventFromNative = GetStaticMethodIDOrDie( env, eventHandlerClass, "postEventFromNative", "(Ljava/lang/Object;IIILjava/lang/Object;)V"); ...... return RegisterMethodsOrDie(env, kEventHandlerClassPathName, gEventHandlerMethods, NELEM(gEventHandlerMethods));&#125; 在android_media_AudioSystem_eventHandlerSetup中会创建JNIAudioPortCallback，然后调用AudioSystem的addAudioPortCallback函数进行注册添加。后续有设备的变化，会通过这个回调sendEvent，进而通过gAudioPortEventHandlerMethods.postEventFromNative返回Java层，通过Handler处理消息。12345678910111213141516171819202122232425262728static voidandroid_media_AudioSystem_eventHandlerSetup(JNIEnv *env, jobject thiz, jobject weak_this)&#123; ALOGV("eventHandlerSetup"); //JNIAudioPortCallback从AudioPortCallback继承 sp&lt;JNIAudioPortCallback&gt; callback = new JNIAudioPortCallback(env, thiz, weak_this); if (AudioSystem::addAudioPortCallback(callback) == NO_ERROR) &#123; setJniCallback(env, thiz, callback); &#125;&#125;void JNIAudioPortCallback::sendEvent(int event)&#123; JNIEnv *env = AndroidRuntime::getJNIEnv(); if (env == NULL) &#123; return; &#125; env-&gt;CallStaticVoidMethod(mClass, gAudioPortEventHandlerMethods.postEventFromNative, mObject, event, 0, 0, NULL); if (env-&gt;ExceptionCheck()) &#123; ALOGW("An exception occurred while notifying an event."); env-&gt;ExceptionClear(); &#125;&#125;void JNIAudioPortCallback::onAudioPortListUpdate()&#123; sendEvent(AUDIOPORT_EVENT_PORT_LIST_UPDATED);&#125; Native层调用AudioSystem的addAudioPortCallback会先获取AudioPolicyService的代理，同时创建AudioPolicyServiceClient，然后将AudioPolicyServiceClient注册到AudioPolicyService，在AudioPolicyService中，以NotificationClien封装AudioPolicyServiceClient，再将其以UID为key保存在mNotificationClients中。然后向AudioPolicyServiceClient中添加回调AudioPortCallback。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061const sp&lt;IAudioPolicyService&gt; AudioSystem::get_audio_policy_service()&#123; sp&lt;IAudioPolicyService&gt; ap; sp&lt;AudioPolicyServiceClient&gt; apc; &#123; Mutex::Autolock _l(gLockAPS); if (gAudioPolicyService == 0) &#123; sp&lt;IServiceManager&gt; sm = defaultServiceManager(); sp&lt;IBinder&gt; binder; do &#123; binder = sm-&gt;getService(String16("media.audio_policy")); if (binder != 0) break; ALOGW("AudioPolicyService not published, waiting..."); usleep(500000); // 0.5 s &#125; while (true); if (gAudioPolicyServiceClient == NULL) &#123; gAudioPolicyServiceClient = new AudioPolicyServiceClient(); &#125; binder-&gt;linkToDeath(gAudioPolicyServiceClient); gAudioPolicyService = interface_cast&lt;IAudioPolicyService&gt;(binder); LOG_ALWAYS_FATAL_IF(gAudioPolicyService == 0); apc = gAudioPolicyServiceClient; // Make sure callbacks can be received by gAudioPolicyServiceClient ProcessState::self()-&gt;startThreadPool(); &#125; ap = gAudioPolicyService; &#125; if (apc != 0) &#123; ap-&gt;registerClient(apc); &#125; return ap;&#125;status_t AudioSystem::addAudioPortCallback(const sp&lt;AudioPortCallback&gt;&amp; callback)&#123; const sp&lt;IAudioPolicyService&gt;&amp; aps = AudioSystem::get_audio_policy_service(); if (aps == 0) return PERMISSION_DENIED; Mutex::Autolock _l(gLockAPS); if (gAudioPolicyServiceClient == 0) &#123; return NO_INIT; &#125; int ret = gAudioPolicyServiceClient-&gt;addAudioPortCallback(callback); if (ret == 1) &#123; aps-&gt;setAudioPortCallbacksEnabled(true); &#125; return (ret &lt; 0) ? INVALID_OPERATION : NO_ERROR;&#125;int AudioSystem::AudioPolicyServiceClient::addAudioPortCallback( const sp&lt;AudioPortCallback&gt;&amp; callback)&#123; Mutex::Autolock _l(mLock); for (size_t i = 0; i &lt; mAudioPortCallbacks.size(); i++) &#123; if (mAudioPortCallbacks[i] == callback) &#123; return -1; &#125; &#125; mAudioPortCallbacks.add(callback); return mAudioPortCallbacks.size();&#125; 如下就是AudioPolicyService保存AudioPolicyServiceClient的代码，我们看到会判断key值uid是否存在，若已经存在，则不会重复保存。我们知道在Android中PID是唯一的，而UID是不唯一的，因为进程之前可以通过sharedUid共享数据。所以从这点看，这里是存在隐患的，假如有两个进程共享UID，且两个进程都会使用getDevices注册回调，这就有可能是的后注册的回调无法得到调用而得不到设备列表更新。12345678910111213141516171819202122232425262728void AudioPolicyService::registerClient(const sp&lt;IAudioPolicyServiceClient&gt;&amp; client)&#123; if (client == 0) &#123; ALOGW("%s got NULL client", __FUNCTION__); return; &#125; Mutex::Autolock _l(mNotificationClientsLock); uid_t uid = IPCThreadState::self()-&gt;getCallingUid(); if (mNotificationClients.indexOfKey(uid) &lt; 0) &#123; sp&lt;NotificationClient&gt; notificationClient = new NotificationClient(this, client, uid); ALOGV("registerClient() client %p, uid %d", client.get(), uid); mNotificationClients.add(uid, notificationClient); sp&lt;IBinder&gt; binder = IInterface::asBinder(client); binder-&gt;linkToDeath(notificationClient); &#125;&#125;void AudioPolicyService::setAudioPortCallbacksEnabled(bool enabled)&#123; Mutex::Autolock _l(mNotificationClientsLock); uid_t uid = IPCThreadState::self()-&gt;getCallingUid(); if (mNotificationClients.indexOfKey(uid) &lt; 0) &#123; return; &#125; mNotificationClients.valueFor(uid)-&gt;setAudioPortCallbacksEnabled(enabled);&#125; 如下是当有设备更新的时候，反向调用回调的过程，设备接入或移除，都会经过AudioPolicyManager的setDeviceConnectionStateInt，然后调用AudioPolicyClient的onAudioPortListUpdate，在AudioPolicyService启动过程我们知道，AudioPolicyClient中的调用几乎最终都会进入AudioPolicyService，使用AudioCommandThread异步处理，调用NotificationClients中的onAudioPortListUpdate，再到AudioPolicyServiceClient的onAudioPortListUpdate，再到JNIAudioPortCallback的onAudioPortListUpdate，调用Java静态函数postEventFromNative，通过Handler发消息执行reset，从而通知所有的client连接的设备已经更新。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576status_t AudioPolicyManager::setDeviceConnectionStateInt(audio_devices_t device, audio_policy_dev_state_t state, const char *device_address, const char *device_name)&#123; ...... if (audio_is_output_device(device)) &#123; SortedVector &lt;audio_io_handle_t&gt; outputs; ssize_t index = mAvailableOutputDevices.indexOf(devDesc); // save a copy of the opened output descriptors before any output is opened or closed // by checkOutputsForDevice(). This will be needed by checkOutputForAllStrategies() mPreviousOutputs = mOutputs; switch (state) &#123; // handle output device connection case AUDIO_POLICY_DEVICE_STATE_AVAILABLE: &#123; ...... index = mAvailableOutputDevices.add(devDesc); ...... case AUDIO_POLICY_DEVICE_STATE_UNAVAILABLE: &#123; ...... default: ALOGE("setDeviceConnectionState() invalid state: %x", state); return BAD_VALUE; &#125; ...... mpClientInterface-&gt;onAudioPortListUpdate(); return NO_ERROR; &#125; // end if is output device // handle input devices if (audio_is_input_device(device)) &#123; ...... &#125; // end if is input device ALOGW("setDeviceConnectionState() invalid device: %x", device); return BAD_VALUE;&#125;void AudioPolicyService::AudioPolicyClient::onAudioPortListUpdate()&#123; mAudioPolicyService-&gt;onAudioPortListUpdate();&#125;void AudioPolicyService::onAudioPortListUpdate()&#123; mOutputCommandThread-&gt;updateAudioPortListCommand();&#125;void AudioPolicyService::doOnAudioPortListUpdate()&#123; Mutex::Autolock _l(mNotificationClientsLock); for (size_t i = 0; i &lt; mNotificationClients.size(); i++) &#123; mNotificationClients.valueAt(i)-&gt;onAudioPortListUpdate(); &#125;&#125;void AudioPolicyService::NotificationClient::onAudioPortListUpdate()&#123; if (mAudioPolicyServiceClient != 0 &amp;&amp; mAudioPortCallbacksEnabled) &#123; mAudioPolicyServiceClient-&gt;onAudioPortListUpdate(); &#125;&#125;void AudioPolicyService::AudioCommandThread::updateAudioPortListCommand()&#123; sp&lt;AudioCommand&gt; command = new AudioCommand(); command-&gt;mCommand = UPDATE_AUDIOPORT_LIST; ALOGV("AudioCommandThread() adding update audio port list"); sendCommand(command);&#125;void AudioSystem::AudioPolicyServiceClient::onAudioPortListUpdate()&#123; Mutex::Autolock _l(mLock); for (size_t i = 0; i &lt; mAudioPortCallbacks.size(); i++) &#123; mAudioPortCallbacks[i]-&gt;onAudioPortListUpdate(); &#125;&#125;]]></content>
      <categories>
        <category>Android Audio</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Audio HAL server启动]]></title>
    <url>%2F2017%2F11%2F28%2FAndroid-Audio-HAL-server%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[我们知道硬件抽象层（HAL）是连接driver和native的桥梁，数据会从native层到HAL层，最终写入kernel。然而8.0之前的HAL和native处于同一进程，耦合度比较高。所以在Android 8.0，google对HAL做了重构，将HAL放在独立的进程，和native通过binder通信。下面我们就看看Android Audio HAL server的启动。 Audio HAL server启动同样Android Audio HAL server（audio-hal-2-0）从init进程启动，不过audio-hal-2-0的可执行文件在vendor分区，这也是为了降低耦合度，因为HAL一般由OEM/ODM实现，google不想HAL影响到Framework的更新，所以希望尽量将OEM/ODM的实现放到vendor分区，由这些厂商自己维护。12345678910111213service audio-hal-2-0 /vendor/bin/hw/android.hardware.audio@2.0-service class hal # audio-hal-2-0和class hal行为一致 user audioserver # 用户归属，uid：AID_AUDIOSERVER # media gid needed for /dev/fm (radio) and for /data/misc/media (tee) group audio camera drmrpc inet media mediadrm net_bt \ net_bt_admin net_bw_acct # 用户组归属 ioprio rt 4 # io调度优先级 # 当子进程被创建的时候，将子进程的pid写入到给定的文件中,cgroup/cpuset用法 writepid /dev/cpuset/foreground/tasks /dev/stune/foreground/tasks # audioflinger restarts itself when it loses connection with the hal # and its .rc file has an "onrestart restart audio-hal" rule, thus # an additional auto-restart from the init process isn't needed. oneshot # 当此服务退出时不会自动重启 我们看到audio-hal-2-0的启动代码有IDevicesFactory，IEffectsFactory，ISoundTriggerHw及IBroadcastRadioFactory，这和native的audioserver相对应。且代码有一种熟悉的感觉，和audioserver的启动很相似，只是具体实现有些不一样，这里通过registerPassthroughServiceImplementation将如上四个服务注册到hwservicemanager，以供native调用。12345678910111213141516171819202122int main(int /* argc */, char* /* argv */ []) &#123; configureRpcThreadpool(16, true /*callerWillJoin*/); android::status_t status; status = registerPassthroughServiceImplementation&lt;IDevicesFactory&gt;(); LOG_ALWAYS_FATAL_IF(status != OK, "Error while registering audio service: %d", status); status = registerPassthroughServiceImplementation&lt;IEffectsFactory&gt;(); LOG_ALWAYS_FATAL_IF(status != OK, "Error while registering audio effects service: %d", status); // Soundtrigger and FM radio might be not present. status = registerPassthroughServiceImplementation&lt;ISoundTriggerHw&gt;(); ALOGE_IF(status != OK, "Error while registering soundtrigger service: %d", status); if (useBroadcastRadioFutureFeatures) &#123; status = registerPassthroughServiceImplementation&lt; broadcastradio::V1_1::IBroadcastRadioFactory&gt;(); &#125; else &#123; status = registerPassthroughServiceImplementation&lt; broadcastradio::V1_0::IBroadcastRadioFactory&gt;(); &#125; ALOGE_IF(status != OK, "Error while registering fm radio service: %d", status); joinRpcThreadpool(); return status;&#125; IDevicesFactory注册到hwservicemanager在如上的启动代码中我们看到，HAL服务端调用registerPassthroughServiceImplementation实现注册，这函数的实现在system/libhidl/transport/include/hidl/LegacySupport.h,这是一个模板方法，这里传入IDevicesFactory接口。1234567891011121314151617/** * Registers passthrough service implementation. */template&lt;class Interface&gt;__attribute__((warn_unused_result))status_t registerPassthroughServiceImplementation( std::string name = "default") &#123; sp&lt;Interface&gt; service = Interface::getService(name, true /* getStub */); ...... status_t status = service-&gt;registerAsService(name); ...... return status;&#125; 所以首先会调用IDevicesFactory的getService方法。在IDevicesFactory.h中看到有getService方法的声明，在DevicesFactoryAll.cpp中有此方法的实现，最后会返回BsDevicesFactory实例。12345678910111213141516171819202122232425262728293031const char* IDevicesFactory::descriptor("android.hardware.audio@2.0::IDevicesFactory");::android::sp&lt;IDevicesFactory&gt; IDevicesFactory::getService( const std::string &amp;serviceName, const bool getStub) &#123; using ::android::hardware::defaultServiceManager; using ::android::hardware::details::waitForHwService; using ::android::hardware::getPassthroughServiceManager; using ::android::hardware::Return; using ::android::sp; using Transport = ::android::hidl::manager::V1_0::IServiceManager::Transport; sp&lt;IDevicesFactory&gt; iface = nullptr; ...... if (getStub || vintfPassthru || vintfLegacy) &#123; const sp&lt;::android::hidl::manager::V1_0::IServiceManager&gt; pm = getPassthroughServiceManager(); if (pm != nullptr) &#123; Return&lt;sp&lt;::android::hidl::base::V1_0::IBase&gt;&gt; ret = pm-&gt;get(IDevicesFactory::descriptor, serviceName); if (ret.isOk()) &#123; sp&lt;::android::hidl::base::V1_0::IBase&gt; baseInterface = ret; if (baseInterface != nullptr) &#123; iface = new BsDevicesFactory(IDevicesFactory::castFrom(baseInterface)); &#125; &#125; &#125; &#125; return iface;&#125; 这里getStub为true，所以前面一段代码不会执行，直接创建PassthroughServiceManager，PassthroughServiceManager的实现在system/libhidl/transport/ServiceManagement.cpp，然后执行其get方法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283Return&lt;sp&lt;IBase&gt;&gt; get(const hidl_string&amp; fqName, const hidl_string&amp; name) override &#123; std::string stdFqName(fqName.c_str()); //fqName looks like android.hardware.foo@1.0::IFoo size_t idx = stdFqName.find("::"); if (idx == std::string::npos || idx + strlen("::") + 1 &gt;= stdFqName.size()) &#123; LOG(ERROR) &lt;&lt; "Invalid interface name passthrough lookup: " &lt;&lt; fqName; return nullptr; &#125; // packageAndVersion为android.hardware.audio@2.0 std::string packageAndVersion = stdFqName.substr(0, idx); // ifaceName为IDevicesFactory std::string ifaceName = stdFqName.substr(idx + strlen("::")); const std::string prefix = packageAndVersion + "-impl"; const std::string sym = "HIDL_FETCH_" + ifaceName; const android_namespace_t* sphal_namespace = android_get_exported_namespace("sphal"); const int dlMode = RTLD_LAZY; void *handle = nullptr; // TODO: lookup in VINTF instead // TODO(b/34135607): Remove HAL_LIBRARY_PATH_SYSTEM dlerror(); // clear // 在/odm/lib(64)/hw/，/vendor/lib(64)/hw/，/system/lib(64)/hw/ // 寻找android.hardware.audio@2.0-impl.so并用dlopen打开，执行 // HIDL_FETCH_IDevicesFactory函数,创建DevicesFactory实例 for (const std::string &amp;path : &#123; HAL_LIBRARY_PATH_ODM, HAL_LIBRARY_PATH_VENDOR, HAL_LIBRARY_PATH_SYSTEM &#125;) &#123; std::vector&lt;std::string&gt; libs = search(path, prefix, ".so"); for (const std::string &amp;lib : libs) &#123; const std::string fullPath = path + lib; // If sphal namespace is available, try to load from the // namespace first. If it fails, fall back to the original // dlopen, which loads from the current namespace. if (sphal_namespace != nullptr &amp;&amp; path != HAL_LIBRARY_PATH_SYSTEM) &#123; const android_dlextinfo dlextinfo = &#123; .flags = ANDROID_DLEXT_USE_NAMESPACE, // const_cast is dirty but required because // library_namespace field is non-const. .library_namespace = const_cast&lt;android_namespace_t*&gt;(sphal_namespace), &#125;; handle = android_dlopen_ext(fullPath.c_str(), dlMode, &amp;dlextinfo); if (handle == nullptr) &#123; const char* error = dlerror(); LOG(WARNING) &lt;&lt; "Failed to dlopen " &lt;&lt; lib &lt;&lt; " from sphal namespace:" &lt;&lt; (error == nullptr ? "unknown error" : error); &#125; else &#123; LOG(DEBUG) &lt;&lt; lib &lt;&lt; " loaded from sphal namespace."; &#125; &#125; if (handle == nullptr) &#123; handle = dlopen(fullPath.c_str(), dlMode); &#125; if (handle == nullptr) &#123; const char* error = dlerror(); LOG(ERROR) &lt;&lt; "Failed to dlopen " &lt;&lt; lib &lt;&lt; ": " &lt;&lt; (error == nullptr ? "unknown error" : error); continue; &#125; IBase* (*generator)(const char* name); *(void **)(&amp;generator) = dlsym(handle, sym.c_str()); if(!generator) &#123; const char* error = dlerror(); LOG(ERROR) &lt;&lt; "Passthrough lookup opened " &lt;&lt; lib &lt;&lt; " but could not find symbol " &lt;&lt; sym &lt;&lt; ": " &lt;&lt; (error == nullptr ? "unknown error" : error); dlclose(handle); continue; &#125; IBase *interface = (*generator)(name.c_str()); if (interface == nullptr) &#123; dlclose(handle); continue; // this module doesn't provide this instance name &#125; registerReference(fqName, name); return interface; &#125; &#125; return nullptr;&#125; 根据android.hardware.audio@2.0::IDevicesFactory字串拼接出android.hardware.audio@2.0-impl.so并找到打开，执行HIDL_FETCH_IDevicesFactory函数创建DevicesFactory返回。得到BsDevicesFactory实例后，会执行其registerAsService。123456789101112::android::status_t IDevicesFactory::registerAsService(const std::string &amp;serviceName) &#123; ::android::hardware::details::onRegistration( "android.hardware.audio@2.0", "IDevicesFactory", serviceName); const ::android::sp&lt;::android::hidl::manager::V1_0::IServiceManager&gt; sm = ::android::hardware::defaultServiceManager(); if (sm == nullptr) &#123; return ::android::INVALID_OPERATION; &#125; ::android::hardware::Return&lt;bool&gt; ret = sm-&gt;add(serviceName.c_str(), this); return ret.isOk() &amp;&amp; ret ? ::android::OK : ::android::UNKNOWN_ERROR;&#125; 获取HwServiceManager将自己注册到hwservicemanager。这里涉及binder通信机制，关于binder通信，是另外一个比较大的topic，在这里一两句说不清楚，所以后续有时间再专门记录。 加载HAL so回想在AudioPolicyService启动的时候，会mDevicesFactoryHal-&gt;openDevice(name, &amp;dev),其实最终会调用DevicesFactory的loadAudioInterface，即打开audio.primary.default.so等lib。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051int DevicesFactory::loadAudioInterface(const char *if_name, audio_hw_device_t **dev)&#123; const hw_module_t *mod; int rc; rc = hw_get_module_by_class(AUDIO_HARDWARE_MODULE_ID, if_name, &amp;mod); if (rc) &#123; ALOGE("%s couldn't load audio hw module %s.%s (%s)", __func__, AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc)); goto out; &#125; rc = audio_hw_device_open(mod, dev); if (rc) &#123; ALOGE("%s couldn't open audio hw device in %s.%s (%s)", __func__, AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc)); goto out; &#125; if ((*dev)-&gt;common.version &lt; AUDIO_DEVICE_API_VERSION_MIN) &#123; ALOGE("%s wrong audio hw device version %04x", __func__, (*dev)-&gt;common.version); rc = -EINVAL; audio_hw_device_close(*dev); goto out; &#125; return OK;out: *dev = NULL; return rc;&#125;// Methods from ::android::hardware::audio::V2_0::IDevicesFactory follow.Return&lt;void&gt; DevicesFactory::openDevice(IDevicesFactory::Device device, openDevice_cb _hidl_cb) &#123; audio_hw_device_t *halDevice; Result retval(Result::INVALID_ARGUMENTS); sp&lt;IDevice&gt; result; const char* moduleName = deviceToString(device); if (moduleName != nullptr) &#123; int halStatus = loadAudioInterface(moduleName, &amp;halDevice); if (halStatus == OK) &#123; if (device == IDevicesFactory::Device::PRIMARY) &#123; result = new PrimaryDevice(halDevice); &#125; else &#123; result = new ::android::hardware::audio::V2_0::implementation:: Device(halDevice); &#125; retval = Result::OK; &#125; else if (halStatus == -EINVAL) &#123; retval = Result::NOT_INITIALIZED; &#125; &#125; _hidl_cb(retval, result); return Void();&#125;]]></content>
      <categories>
        <category>Android Audio HAL</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Audio HIDL C++]]></title>
    <url>%2F2017%2F11%2F24%2FAndroid-Audio-HIDL-C%2B%2B%2F</url>
    <content type="text"><![CDATA[HAL接口定义语言（简称HIDL，发音为“hide-l”）是用于指定HAL和其用户之间的接口的一种接口描述语言 (IDL)。HIDL允许指定类型和方法调用（会汇集到接口和软件包中）。从更广泛的意义上来说，HIDL是用于在可以独立编译的代码库之间进行通信的系统。 HIDL旨在用于进程间通信 (IPC)。进程之间的通信经过Binder化。对于必须与进程相关联的代码库，还可以使用直通模式（在Java中不受支持）。 HIDL可指定数据结构和方法签名，这些内容会整理归类到接口（与类相似）中，而接口会汇集到软件包中。尽管HIDL具有一系列不同的关键字，但C++和Java程序员对HIDL的语法并不陌生。此外，HIDL还使用Java样式的注释。 HAL分类为了更好地实现模块化，Android 8.0对Android操作系统底层进行了重新架构。作为此变化的一部分，运行 Android 8.0的设备必须支持绑定式或直通式HAL： 绑定式HAL：以HAL接口定义语言 (HIDL) 表示的HAL。这些HAL取代了早期 Android 版本中使用的传统 HAL和旧版HAL。在绑定式HAL中，Android框架和HAL之间通过Binder进程间通信 (IPC) 调用进行通信。所有在推出时即搭载了Android 8.0或后续版本的设备都必须只支持绑定式HAL。 直通式HAL：以HIDL封装的传统HAL或旧版HAL。这些HAL封装了现有的HAL，可在绑定模式和 Same-Process（直通）模式下使用。升级到 Android 8.0 的设备可以使用直通式HAL。 HAL的发展历程HIDL接口具有客户端和服务器实现： HIDL接口的客户端实现是指通过在该接口上调用方法来使用该接口的代码。 服务器实现是指HIDL接口的实现，它可接收来自客户端的调用并返回结果（如有必要）。 在从libhardware HAL转换为HIDL HAL的过程中，HAL实现成为服务器，而调用HAL的进程则成为客户端。默认实现可提供直通和绑定式HAL，并可能会随着时间而发生变化： Audio HIDL客户端和服务端客户端创建libaudioflinger.so会依赖libaudiohal.so12LOCAL_SHARED_LIBRARIES += \ libaudiohal libaudiohal.so依赖android.hardware.audio@2.0.so12LOCAL_SHARED_LIBRARIES += \ android.hardware.audio@2.0 记得在AudioFlinger的启动过程中，在创建libaudiohal的DevicesFactoryHalHidl时有如下以一段代码，这里就是创建Audio HIDL的客户端。123456#include &lt;android/hardware/audio/2.0/IDevicesFactory.h&gt;DevicesFactoryHalHidl::DevicesFactoryHalHidl() &#123; mDevicesFactory = IDevicesFactory::getService(); ......&#125; 服务端创建生成可执行文件android.hardware.audio@2.0-service代表服务端，同样依赖于android.hardware.audio@2.0123LOCAL_MODULE := android.hardware.audio@2.0-serviceLOCAL_SHARED_LIBRARIES += \ android.hardware.audio@2.0 为了让HAL在直通模式下发挥作用（对于旧版设备），您必须具有HIDL_FETCH_IModuleName 函数（位于/system/lib(64)?/hw/android.hardware.package@3.0-impl(-$OPTIONAL_IDENTIFIER).so下），其中$OPTIONAL_IDENTIFIER是一个标识直通实现的字符串。比如对于Audio,在android.hardware.audio@2.0-impl.so中：123IDevicesFactory* HIDL_FETCH_IDevicesFactory(const char* /* name */) &#123; return new DevicesFactory();&#125; 接下来，使用功能填写存根并设置守护进程。守护进程代码（支持直通）示例：12345#include &lt;hidl/LegacySupport.h&gt;int main(int /* argc */, char* /* argv */ []) &#123; status = registerPassthroughServiceImplementation&lt;IDevicesFactory&gt;();&#125; registerPassthroughServiceImplementation将对提供的-impl库执行dlopen()操作，并将其作为绑定式服务提供。守护进程代码（对于纯绑定式服务）示例：1234int main(int /* argc */, char* /* argv */ []) &#123; Nfc nfc = new Nfc(); nfc-&gt;registerAsService();&#125; 此守护进程应该存在于$PACKAGE + “-service”（例如android.hardware.audio@2.0-service）中。HAL的特定类的sepolicy是属性hal_（例如 hal_audio))。您必须将此属性应用到运行特定HAL 的守护进程（如果同一进程提供多个HAL，则可以将多个属性应用到该进程）。 Audio HIDL接口软件包HIDL接口软件包位于hardware/interfaces或vendor/目录下（少数例外情况除外）。hardware/interfaces顶层会直接映射到android.hardware软件包命名空间；版本是软件包（而不是接口）命名空间下的子目录。 hidl-gen编译器会将.hal文件编译成一组.h和.cpp文件。这些自动生成的文件可用来编译客户端/服务器实现链接到的共享库。用于编译此共享库的Android.bp文件由hardware/interfaces/update-makefiles.sh 脚本自动生成。每次将新软件包添加到hardware/interfaces或在现有软件包中添加/移除.hal文件时，您都必须重新运行该脚本，以确保生成的共享库是最新的。 Auido HIDL接口定义对于Audio，.hal文件位于hardware/interfaces/audio/2.0下，我们看看客户端和服务端同时使用的IDevicesFactory接口，该定义在IDevicesFactory.hal中。1234567891011121314151617181920212223242526272829package android.hardware.audio@2.0;import android.hardware.audio.common@2.0;import IDevice;interface IDevicesFactory &#123; typedef android.hardware.audio@2.0::Result Result; enum Device : int32_t &#123; PRIMARY, A2DP, USB, R_SUBMIX, STUB &#125;; /** * Opens an audio device. To close the device, it is necessary to release * references to the returned device object. * * @param device device type. * @return retval operation completion status. Returns INVALID_ARGUMENTS * if there is no corresponding hardware module found, * NOT_INITIALIZED if an error occured while opening the hardware * module. * @return result the interface for the created device. */ openDevice(Device device) generates (Result retval, IDevice result);&#125;; HIDL接口转化为cpp如下是编译android.hardware.audio@2.0的Android.bp,由hidl-gen自动产生，不能手动编辑。IDevicesFactory.hal会生成DevicesFactoryAll.cpp，及IDevicesFactory.h，IHwDevicesFactory.h，BnHwDevicesFactory.h，BpHwDevicesFactory.h，BsDevicesFactory.h。生成的.h位于out/soong/.intermediates/hardware/interfaces/audio/2.0/android.hardware.audio@2.0_genc++_headers，生成的.cpp文件位于out/soong/.intermediates/hardware/interfaces/audio/2.0/android.hardware.audio@2.0_genc++。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// This file is autogenerated by hidl-gen. Do not edit manually.filegroup &#123; name: "android.hardware.audio@2.0_hal", srcs: [ "types.hal", "IDevice.hal", "IDevicesFactory.hal", "IPrimaryDevice.hal", "IStream.hal", "IStreamIn.hal", "IStreamOut.hal", "IStreamOutCallback.hal", ],&#125;genrule &#123; name: "android.hardware.audio@2.0_genc++", tools: ["hidl-gen"], cmd: "$(location hidl-gen) -o $(genDir) -Lc++-sources -randroid.hardware:hardware/interfaces -randroid.hidl:system/libhidl/transport android.hardware.audio@2.0", srcs: [ ":android.hardware.audio@2.0_hal", ], out: [ "android/hardware/audio/2.0/types.cpp", "android/hardware/audio/2.0/DeviceAll.cpp", "android/hardware/audio/2.0/DevicesFactoryAll.cpp", "android/hardware/audio/2.0/PrimaryDeviceAll.cpp", "android/hardware/audio/2.0/StreamAll.cpp", "android/hardware/audio/2.0/StreamInAll.cpp", "android/hardware/audio/2.0/StreamOutAll.cpp", "android/hardware/audio/2.0/StreamOutCallbackAll.cpp", ],&#125;genrule &#123; name: "android.hardware.audio@2.0_genc++_headers", tools: ["hidl-gen"], cmd: "$(location hidl-gen) -o $(genDir) -Lc++-headers -randroid.hardware:hardware/interfaces -randroid.hidl:system/libhidl/transport android.hardware.audio@2.0", srcs: [ ":android.hardware.audio@2.0_hal", ], out: [ "android/hardware/audio/2.0/types.h", "android/hardware/audio/2.0/hwtypes.h", "android/hardware/audio/2.0/IDevice.h", "android/hardware/audio/2.0/IHwDevice.h", "android/hardware/audio/2.0/BnHwDevice.h", "android/hardware/audio/2.0/BpHwDevice.h", "android/hardware/audio/2.0/BsDevice.h", "android/hardware/audio/2.0/IDevicesFactory.h", "android/hardware/audio/2.0/IHwDevicesFactory.h", "android/hardware/audio/2.0/BnHwDevicesFactory.h", "android/hardware/audio/2.0/BpHwDevicesFactory.h", "android/hardware/audio/2.0/BsDevicesFactory.h", "android/hardware/audio/2.0/IPrimaryDevice.h", "android/hardware/audio/2.0/IHwPrimaryDevice.h", "android/hardware/audio/2.0/BnHwPrimaryDevice.h", "android/hardware/audio/2.0/BpHwPrimaryDevice.h", "android/hardware/audio/2.0/BsPrimaryDevice.h", "android/hardware/audio/2.0/IStream.h", "android/hardware/audio/2.0/IHwStream.h", "android/hardware/audio/2.0/BnHwStream.h", "android/hardware/audio/2.0/BpHwStream.h", "android/hardware/audio/2.0/BsStream.h", "android/hardware/audio/2.0/IStreamIn.h", "android/hardware/audio/2.0/IHwStreamIn.h", "android/hardware/audio/2.0/BnHwStreamIn.h", "android/hardware/audio/2.0/BpHwStreamIn.h", "android/hardware/audio/2.0/BsStreamIn.h", "android/hardware/audio/2.0/IStreamOut.h", "android/hardware/audio/2.0/IHwStreamOut.h", "android/hardware/audio/2.0/BnHwStreamOut.h", "android/hardware/audio/2.0/BpHwStreamOut.h", "android/hardware/audio/2.0/BsStreamOut.h", "android/hardware/audio/2.0/IStreamOutCallback.h", "android/hardware/audio/2.0/IHwStreamOutCallback.h", "android/hardware/audio/2.0/BnHwStreamOutCallback.h", "android/hardware/audio/2.0/BpHwStreamOutCallback.h", "android/hardware/audio/2.0/BsStreamOutCallback.h", ],&#125;cc_library_shared &#123; name: "android.hardware.audio@2.0", defaults: ["hidl-module-defaults"], generated_sources: ["android.hardware.audio@2.0_genc++"], generated_headers: ["android.hardware.audio@2.0_genc++_headers"], export_generated_headers: ["android.hardware.audio@2.0_genc++_headers"], vendor_available: true, shared_libs: [ "libhidlbase", "libhidltransport", "libhwbinder", "liblog", "libutils", "libcutils", "android.hardware.audio.common@2.0", ], export_shared_lib_headers: [ "libhidlbase", "libhidltransport", "libhwbinder", "libutils", "android.hardware.audio.common@2.0", ],&#125; IDevicesFactory.h - 描述C++类中的纯IDevicesFactory接口；它包含IDevicesFactory.hal文件中的IDevicesFactory接口中所定义的方法和类型，必要时会转换为C++类型。不包含与用于实现此接口的RPC机制（例如HwBinder）相关的详细信息。类的命名空间包含软件包名称和版本号，例如::android::hardware::audio::V2_0::IDevicesFactory。客户端和服务器都包含此标头：客户端用它来调用方法，服务器用它来实现这些方法。 IHwDevicesFactory.h - 头文件，其中包含用于对接口中使用的数据类型进行序列化的函数的声明。开发者不得直接包含其标头（它不包含任何类） BpHwDevicesFactory.h - 从IDevicesFactory继承的类，可描述接口的HwBinder代理（客户端）实现。开发者不得直接引用此类。 BnHwDevicesFactory.h - 保存对IDevicesFactory实现的引用的类，可描述接口的HwBinder存根（服务器端）实现。开发者不得直接引用此类。 DevicesFactoryAll.cpp - 包含HwBinder代理和HwBinder存根的实现的类。当客户端调用接口方法时，代理会自动从客户端封送参数，并将事务发送到绑定内核驱动程序，该内核驱动程序会将事务传送到另一端的存根（该存根随后会调用实际的服务器实现） BsDevicesFactory.h - 从IDevicesFactory继承的类，直通模式对IDevicesFactory的服务端实现。开发者不得直接引用此类。 这些文件的结构类似于由aidl-cpp生成的文件。独立于HIDL使用的RPC机制的唯一一个自动生成的文件是 IDevicesFactory.h，其他所有文件都与HIDL使用的HwBinder RPC机制相关联。因此，客户端和服务器实现不得直接引用除IDevicesFactory之外的任何内容。为了满足这项要求，请只包含IDevicesFactory.h并链接到生成的共享库。]]></content>
      <categories>
        <category>Android Audio HAL</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AudioPolicyService启动过程]]></title>
    <url>%2F2017%2F11%2F10%2FAudioPolicyService%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[AudioPolicyService在Audio系统另一个重要的服务，是音频策略的制定者，负责音频设备切换的策略抉择、音量调节策略等。本文基于Android 8.0的代码，记录了AudioPolicyService启动的过程，介绍了其中几个比较关键的点，希望以后自己看到此文能快速回想起AudioPolicyService的启动过程。其代码位于frameworks/av/services/audiopolicy。 audiopolicy的代码结构 service目录是AudioPolicyService、AudioPolicyClient、AudioCommandThread及AudioPolicyEffects的定义及实现。 managerdefault目录提供AudioPolicyManager的基本实现。 manager目录中是一个工厂类可以根据需要生产所需的AudioPolicyManager，一般各个厂商都会自己实现自己的AudioPolicyManager。 engine目录定义了AudioPolicyManagerInterface和AudioPolicyManagerObserver接口，AudioPolicyManagerInterface由Policy Engine实现，AudioPolicyManagerObserver这个观察者由AudioPolicyManager实现，以供Engine访问。 engineconfigure和enginedefault目录是Policy Engine的两种实现，可以根据需要选择其中一种。 config目录存放audio policy及音量曲线的config文件。 common目录下定义公共代码。audio_policy.conf是旧版本的audio policy config文件。 AudioPolicyInterface.h定义了AudioPolicyInterface和AudioPolicyClientInterface接口，AudioPolicyInterface由AuidoPolicyManager实现，特定平台AuidoPolicyManager通过AudioPolicyClientInterface接口的实现者AudioPolicyClient控制音频的输入输出。AudioPolicyService初始化从Android 8.0的code发现，基本流程还是和之前一样，分别创建了ApmTone、ApmAudio、ApmOutput三个AudioCommandThread线程，分别用于播放tone音、执行audio命令和执行输出命令，创建AudioPolicyClient，创建AudioPolicyManager以及创建AudioPolicyEffects。但已经完全移除了对旧版本的AUDIO_POLICY_HARDWARE_MODULE_ID的支持（不再加载audio_policy.default.so库得到audio_policy_module模块），完全使用新模式。12345678910111213141516171819202122232425262728AudioPolicyService::AudioPolicyService() : BnAudioPolicyService(), mpAudioPolicyDev(NULL), mpAudioPolicy(NULL), mAudioPolicyManager(NULL), mAudioPolicyClient(NULL), mPhoneState(AUDIO_MODE_INVALID)&#123;&#125;void AudioPolicyService::onFirstRef()&#123; &#123; Mutex::Autolock _l(mLock); // start tone playback thread mTonePlaybackThread = new AudioCommandThread(String8("ApmTone"), this); // start audio commands thread mAudioCommandThread = new AudioCommandThread(String8("ApmAudio"), this); // start output activity command thread mOutputCommandThread = new AudioCommandThread(String8("ApmOutput"), this); mAudioPolicyClient = new AudioPolicyClient(this); mAudioPolicyManager = createAudioPolicyManager(mAudioPolicyClient); &#125; // load audio processing modules sp&lt;AudioPolicyEffects&gt;audioPolicyEffects = new AudioPolicyEffects(); &#123; Mutex::Autolock _l(mLock); mAudioPolicyEffects = audioPolicyEffects; &#125;&#125; 这些类之间的大致关系如下（AudioPolicyClient和AudioCommandThread都为AudioPolicyService的内部类）：AudioPolicyService的初始化大致分为3步：1.创建三个AudioCommandThread（mTonePlaybackThread，mAudioCommandThread和mOutputCommandThread）2.初始化AudioPolicyManager 2.1 加载并解析audio_policy_configuration.xml 2.2 加载对应的HW module 2.3 初始化Policy Engine 2.4 打开输入输出 2.5 确保所有可用输入输出设备和默认输出设备真正可用3.初始化AudioPolicyEffects AudioCommandThread线程AudioCommandThread采用异步方式来执行audio command，当需要执行上表中的命令时，首先将命令投递到AudioCommandThread的mAudioCommands命令向量表中，然后通过mWaitWorkCV.signal()唤醒AudioCommandThread线程，被唤醒的AudioCommandThread线程执行完command后，又通过mWaitWorkCV.waitRelative(mLock, waitTime)睡眠等待命令到来。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879void AudioPolicyService::AudioCommandThread::onFirstRef()&#123; run(mName.string(), ANDROID_PRIORITY_AUDIO);&#125;status_t AudioPolicyService::AudioCommandThread::volumeCommand(audio_stream_type_t stream, float volume, audio_io_handle_t output, int delayMs)&#123; sp&lt;AudioCommand&gt; command = new AudioCommand(); command-&gt;mCommand = SET_VOLUME; sp&lt;VolumeData&gt; data = new VolumeData(); data-&gt;mStream = stream; data-&gt;mVolume = volume; data-&gt;mIO = output; command-&gt;mParam = data; command-&gt;mWaitStatus = true; return sendCommand(command, delayMs);&#125;bool AudioPolicyService::AudioCommandThread::threadLoop()&#123; nsecs_t waitTime = -1; mLock.lock(); while (!exitPending()) &#123; sp&lt;AudioPolicyService&gt; svc; while (!mAudioCommands.isEmpty() &amp;&amp; !exitPending()) &#123; nsecs_t curTime = systemTime(); // commands are sorted by increasing time stamp: execute them from index 0 and up if (mAudioCommands[0]-&gt;mTime &lt;= curTime) &#123; sp&lt;AudioCommand&gt; command = mAudioCommands[0]; mAudioCommands.removeAt(0); mLastCommand = command; switch (command-&gt;mCommand) &#123; ...... case SET_VOLUME: &#123; VolumeData *data = (VolumeData *)command-&gt;mParam.get(); ALOGV("AudioCommandThread() processing set volume stream %d, \ volume %f, output %d", data-&gt;mStream, data-&gt;mVolume, data-&gt;mIO); command-&gt;mStatus = AudioSystem::setStreamVolume(data-&gt;mStream, data-&gt;mVolume, data-&gt;mIO); &#125;break; ...... &#125; ..... &#125; else &#123; waitTime = mAudioCommands[0]-&gt;mTime - curTime; break; &#125; &#125; ...... // At this stage we have either an empty command queue or the first command in the queue // has a finite delay. So unless we are exiting it is safe to wait. if (!exitPending()) &#123; ALOGV("AudioCommandThread() going to sleep"); if (waitTime == -1) &#123; mWaitWorkCV.wait(mLock); &#125; else &#123; mWaitWorkCV.waitRelative(mLock, waitTime); &#125; &#125; &#125; ...... mLock.unlock(); return false;&#125; AudioCommandThread是AudioPolicyService的内部类，AudioCommandThread在内部又定义了AudioCommand及AudioCommandData关系如下。 AudioPolicyManager初始化12345678910111213141516171819202122232425262728293031323334AudioPolicyManager::AudioPolicyManager(AudioPolicyClientInterface *clientInterface) : mLimitRingtoneVolume(false), mLastVoiceVolume(-1.0f), mA2dpSuspended(false), mAudioPortGeneration(1), mBeaconMuteRefCount(0), mBeaconPlayingRefCount(0), mBeaconMuted(false), mTtsOutputAvailable(false), mMasterMono(false), mMusicEffectOutput(AUDIO_IO_HANDLE_NONE), mHasComputedSoundTriggerSupportsConcurrentCapture(false)&#123; mUidCached = getuid(); mpClientInterface = clientInterface; ...... // audio policy config加载 // 初始化Policy Engine for (size_t i = 0; i &lt; mHwModules.size(); i++) &#123; // 加载所有的HW module ...... // 打开音频输入输出 &#125; // 确保所有可用输入输出设备真正可用 ...... // 确保默认输出设备真正可用 ALOGE_IF((mPrimaryOutput == 0), "Failed to open primary output"); updateDevicesAndOutputs();&#125; audio policy config加载进入AudioPolicyManager构造函数，会首先加载audio policy config文件，对于旧版本使用audio_policy.conf，在代码中定义音量曲线，对于新版本使用audio_policy_configuration.xml。对于Android 8.0使用新版本，这个文件一般会位于/odm/etc或/vendor/etc，同时解析音量曲线xml（audio_policy_volumes.xml和default_volume_tables.xml）,比起之前的硬编码音量曲线，灵活性更好。1234567891011121314151617181920#ifdef USE_XML_AUDIO_POLICY_CONF mVolumeCurves = new VolumeCurvesCollection(); AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices, mDefaultOutputDevice, speakerDrcEnabled, static_cast&lt;VolumeCurvesCollection *&gt;(mVolumeCurves)); if (deserializeAudioPolicyXmlConfig(config) != NO_ERROR) &#123;#else mVolumeCurves = new StreamDescriptorCollection(); AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices, mDefaultOutputDevice, speakerDrcEnabled); if ((ConfigParsingUtils::loadConfig(AUDIO_POLICY_VENDOR_CONFIG_FILE, config) != NO_ERROR) &amp;&amp; (ConfigParsingUtils::loadConfig(AUDIO_POLICY_CONFIG_FILE, config) != NO_ERROR)) &#123;#endif ALOGE("could not load audio policy configuration file, setting defaults"); config.setDefault(); &#125; // must be done after reading the policy (since conditionned by Speaker Drc Enabling) // xml模式时这里是一个空函数无实现 mVolumeCurves-&gt;initializeVolumeCurves(speakerDrcEnabled); 当执行完deserializeAudioPolicyXmlConfig，会得到mHwModules对应各个Audio HAL模块、mAvailableOutputDevices可用的输出设备、mAvailableInputDevices可用的输入设备、mDefaultOutputDevice默认输出设备、mVolumeCurves音量曲线及speakerDrcEnabled。 audio_policy_configuration.xml同时定义了多个audio接口(HwModules)，每一个audio接口包含若干routes（通路）、devicesPorts（设备）和mixPorts（音频流），而每个mixPorts又包含多个input和output流，每个input和output流又同时支持多种输入输出模式，每种输入输出模式又支持若干种设备。mixPorts：listing all output and input streams exposed by the audio HAL.routes：list of possible connections between input and output devices or between stream and devices.devicePorts：a list of device descriptors for all input and output devices accessible via this module.This contains both permanently attached devices and removable devices.每个stream type会分为四种device category：DEVICE_CATEGORY_HEADSET，DEVICE_CATEGORY_SPEAKER，DEVICE_CATEGORY_EARPIECE及DEVICE_CATEGORY_EXT_MEDIA来定义音量曲线，定义的形式如下，一样是分段定义，在每一段中定义不同的衰减值以控制音量的大小。It contains a list of points of this curve expressing the attenuation in Millibels for a given volume index from 0 to 100.123456&lt;volume stream="AUDIO_STREAM_VOICE_CALL" deviceCategory="DEVICE_CATEGORY_HEADSET"&gt; &lt;point&gt;0,-4200&lt;/point&gt; &lt;point&gt;33,-2800&lt;/point&gt; &lt;point&gt;66,-1400&lt;/point&gt; &lt;point&gt;100,0&lt;/point&gt;&lt;/volume&gt; 初始化Policy Engine这里也算一个观察者模式吧，EngineInstance是单例模式，通过EngineInstance创建AudioPolicyManagerInterface，从而创建Policy Engine，然后设置观察者。12345678910111213141516// Once policy config has been parsed, retrieve an instance of the engine and initialize it. audio_policy::EngineInstance *engineInstance = audio_policy::EngineInstance::getInstance(); if (!engineInstance) &#123; ALOGE("%s: Could not get an instance of policy engine", __FUNCTION__); return; &#125; // Retrieve the Policy Manager Interface mEngine = engineInstance-&gt;queryInterface&lt;AudioPolicyManagerInterface&gt;(); if (mEngine == NULL) &#123; ALOGE("%s: Failed to get Policy Engine Interface", __FUNCTION__); return; &#125; mEngine-&gt;setObserver(this); status_t status = mEngine-&gt;initCheck(); (void) status; ALOG_ASSERT(status == NO_ERROR, "Policy engine not initialized(err=%d)", status); 其大致关系如下。 加载HW Module根据audio policy config加载的HwModule真正加载HAL层的HW module。123456789101112// mAvailableOutputDevices and mAvailableInputDevices now contain all attached devices// open all output streams needed to access attached devicesaudio_devices_t outputDeviceTypes = mAvailableOutputDevices.types();audio_devices_t inputDeviceTypes = mAvailableInputDevices.types() &amp; ~AUDIO_DEVICE_BIT_IN;for (size_t i = 0; i &lt; mHwModules.size(); i++) &#123; mHwModules[i]-&gt;mHandle = mpClientInterface-&gt;loadHwModule(mHwModules[i]-&gt;getName()); if (mHwModules[i]-&gt;mHandle == 0) &#123; continue; &#125; ......&#125; 从如上代码看出，会执行mpClientInterface-&gt;loadHwModule，即调用AudioPolicyClient的loadHwModule函数，又会转到AudioFlinger的loadHwModule。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859audio_module_handle_t AudioPolicyService::AudioPolicyClient::loadHwModule(const char *name)&#123; sp&lt;IAudioFlinger&gt; af = AudioSystem::get_audio_flinger(); if (af == 0) &#123; ALOGW("%s: could not get AudioFlinger", __func__); return AUDIO_MODULE_HANDLE_NONE; &#125; return af-&gt;loadHwModule(name);&#125;audio_module_handle_t AudioFlinger::loadHwModule(const char *name)&#123; if (name == NULL) &#123; return AUDIO_MODULE_HANDLE_NONE; &#125; if (!settingsAllowed()) &#123; return AUDIO_MODULE_HANDLE_NONE; &#125; Mutex::Autolock _l(mLock); return loadHwModule_l(name);&#125;// loadHwModule_l() must be called with AudioFlinger::mLock heldaudio_module_handle_t AudioFlinger::loadHwModule_l(const char *name)&#123; for (size_t i = 0; i &lt; mAudioHwDevs.size(); i++) &#123; if (strncmp(mAudioHwDevs.valueAt(i)-&gt;moduleName(), name, strlen(name)) == 0) &#123; ALOGW("loadHwModule() module %s already loaded", name); return mAudioHwDevs.keyAt(i); &#125; &#125; sp&lt;DeviceHalInterface&gt; dev; int rc = mDevicesFactoryHal-&gt;openDevice(name, &amp;dev); if (rc) &#123; ALOGE("loadHwModule() error %d loading module %s", rc, name); return AUDIO_MODULE_HANDLE_NONE; &#125; ...... // Check and cache this HAL's level of support for master mute and master // volume. If this is the first HAL opened, and it supports the get // methods, use the initial values provided by the HAL as the current // master mute and volume settings. ...... audio_module_handle_t handle = (audio_module_handle_t) nextUniqueId(AUDIO_UNIQUE_ID_USE_MODULE); mAudioHwDevs.add(handle, new AudioHwDevice(handle, name, dev, flags)); ALOGI("loadHwModule() Loaded %s audio interface, handle %d", name, handle); return handle;&#125; mDevicesFactoryHal-&gt;openDevice(name, &amp;dev),从前面AudioFlinger的启动，我们知道mDevicesFactoryHal是HAL进程的客户端，对于Android 8.0加载HAL so文件已经移到HAL进程中，不再与audioserver处于同一个进程中。在AudioFlinger中使用AudioHwDevice代表HW Module，AudioHwDevice会封装audio_module_handle_t和DeviceHalInterface，并以audio_module_handle_t为key将其保存在AudioFlinger的mAudioHwDevs中，以供后续查询。到这里就加载系统的音频接口就加载完了，我们大致可以得出如下结果。 打开音频输出这里的输出，即mixPorts中outputs，也就是mHwModules中所有OutputProfile（IOProfile），代表了音频输出流。所以会遍历mHwModules中所有OutputProfile，然后SwAudioOutputDescriptor来描述每一个output，最终保存在以audio_io_handle_t为key的mOutputs中，以供后续查询。不过这里会除了AUDIO_OUTPUT_FLAG_DIRECT，AUDIO_OUTPUT_FLAG_DIRECT的output会在使用的时候打开，不会预先open。在打开输出设备后还会标记可用输出设备的可用情况，以备后续确认可用输出设备真正可用。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// open all output streams needed to access attached devices// except for direct output streams that are only opened when they are actually// required by an app.// This also validates mAvailableOutputDevices listfor (size_t j = 0; j &lt; mHwModules[i]-&gt;mOutputProfiles.size(); j++)&#123; const sp&lt;IOProfile&gt; outProfile = mHwModules[i]-&gt;mOutputProfiles[j]; if (!outProfile-&gt;hasSupportedDevices()) &#123; continue; &#125; if ((outProfile-&gt;getFlags() &amp; AUDIO_OUTPUT_FLAG_TTS) != 0) &#123; mTtsOutputAvailable = true; &#125; if ((outProfile-&gt;getFlags() &amp; AUDIO_OUTPUT_FLAG_DIRECT) != 0) &#123; continue; &#125; audio_devices_t profileType = outProfile-&gt;getSupportedDevicesType(); if ((profileType &amp; mDefaultOutputDevice-&gt;type()) != AUDIO_DEVICE_NONE) &#123; profileType = mDefaultOutputDevice-&gt;type(); &#125; else &#123; // chose first device present in profile's SupportedDevices also part of // outputDeviceTypes profileType = outProfile-&gt;getSupportedDeviceForType(outputDeviceTypes); &#125; if ((profileType &amp; outputDeviceTypes) == 0) &#123; continue; &#125; sp&lt;SwAudioOutputDescriptor&gt; outputDesc = new SwAudioOutputDescriptor(outProfile, mpClientInterface); const DeviceVector &amp;supportedDevices = outProfile-&gt;getSupportedDevices(); const DeviceVector &amp;devicesForType = supportedDevices.getDevicesFromType(profileType); String8 address = devicesForType.size() &gt; 0 ? devicesForType.itemAt(0)-&gt;mAddress : String8(""); outputDesc-&gt;mDevice = profileType; audio_config_t config = AUDIO_CONFIG_INITIALIZER; config.sample_rate = outputDesc-&gt;mSamplingRate; config.channel_mask = outputDesc-&gt;mChannelMask; config.format = outputDesc-&gt;mFormat; audio_io_handle_t output = AUDIO_IO_HANDLE_NONE; status_t status = mpClientInterface-&gt;openOutput(outProfile-&gt;getModuleHandle(), &amp;output, &amp;config, &amp;outputDesc-&gt;mDevice, address, &amp;outputDesc-&gt;mLatency, outputDesc-&gt;mFlags); if (status != NO_ERROR) &#123; ...... &#125; else &#123; outputDesc-&gt;mSamplingRate = config.sample_rate; outputDesc-&gt;mChannelMask = config.channel_mask; outputDesc-&gt;mFormat = config.format; for (size_t k = 0; k &lt; supportedDevices.size(); k++) &#123; ssize_t index = mAvailableOutputDevices.indexOf(supportedDevices[k]); // give a valid ID to an attached device once confirmed it is reachable if (index &gt;= 0 &amp;&amp; !mAvailableOutputDevices[index]-&gt;isAttached()) &#123; mAvailableOutputDevices[index]-&gt;attach(mHwModules[i]); &#125; &#125; if (mPrimaryOutput == 0 &amp;&amp; outProfile-&gt;getFlags() &amp; AUDIO_OUTPUT_FLAG_PRIMARY) &#123; mPrimaryOutput = outputDesc; &#125; addOutput(output, outputDesc); setOutputDevice(outputDesc, outputDesc-&gt;mDevice, true, 0, NULL, address.string()); &#125;&#125; 我们看到会使用到mpClientInterface打开输出，即调用AudioPolicyClient的openOutput，即调用AudioFlinger的openOutput及openOutput_l。首先调用findSuitableHwDev_l查询合适的AudioHwDevice，即通过audio_module_handle_t在之前加载的mAudioHwDevs中去除对应的AudioHwDevice，调用AudioHwDevice的openOutputStream得到AudioStreamOut，然后根据output flag创建相应的Thread，最后以audio_io_handle_t为key将Thread保存在mPlaybackThreads和mMmapThreads。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798sp&lt;AudioFlinger::ThreadBase&gt; AudioFlinger::openOutput_l(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t devices, const String8&amp; address, audio_output_flags_t flags)&#123; AudioHwDevice *outHwDev = findSuitableHwDev_l(module, devices); if (outHwDev == NULL) &#123; return 0; &#125; if (*output == AUDIO_IO_HANDLE_NONE) &#123; *output = nextUniqueId(AUDIO_UNIQUE_ID_USE_OUTPUT); &#125; else &#123; return 0; &#125; mHardwareStatus = AUDIO_HW_OUTPUT_OPEN; // FOR TESTING ONLY: ...... AudioStreamOut *outputStream = NULL; status_t status = outHwDev-&gt;openOutputStream( &amp;outputStream, *output, devices, flags, config, address.string()); mHardwareStatus = AUDIO_HW_IDLE; if (status == NO_ERROR) &#123; if (flags &amp; AUDIO_OUTPUT_FLAG_MMAP_NOIRQ) &#123; sp&lt;MmapPlaybackThread&gt; thread = new MmapPlaybackThread(this, *output, outHwDev, outputStream, devices, AUDIO_DEVICE_NONE, mSystemReady); mMmapThreads.add(*output, thread); ALOGV("openOutput_l() created mmap playback thread: ID %d thread %p", *output, thread.get()); return thread; &#125; else &#123; sp&lt;PlaybackThread&gt; thread; if (flags &amp; AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD) &#123; thread = new OffloadThread(this, outputStream, *output, devices, mSystemReady); ALOGV("openOutput_l() created offload output: ID %d thread %p", *output, thread.get()); &#125; else if ((flags &amp; AUDIO_OUTPUT_FLAG_DIRECT) || !isValidPcmSinkFormat(config-&gt;format) || !isValidPcmSinkChannelMask(config-&gt;channel_mask)) &#123; thread = new DirectOutputThread(this, outputStream, *output, devices, mSystemReady); ALOGV("openOutput_l() created direct output: ID %d thread %p", *output, thread.get()); &#125; else &#123; thread = new MixerThread(this, outputStream, *output, devices, mSystemReady); ALOGV("openOutput_l() created mixer output: ID %d thread %p", *output, thread.get()); &#125; mPlaybackThreads.add(*output, thread); return thread; &#125; &#125; return 0;&#125;status_t AudioFlinger::openOutput(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t *devices, const String8&amp; address, uint32_t *latencyMs, audio_output_flags_t flags)&#123; if (devices == NULL || *devices == AUDIO_DEVICE_NONE) &#123; return BAD_VALUE; &#125; Mutex::Autolock _l(mLock); sp&lt;ThreadBase&gt; thread = openOutput_l(module, output, config, *devices, address, flags); if (thread != 0) &#123; if ((flags &amp; AUDIO_OUTPUT_FLAG_MMAP_NOIRQ) == 0) &#123; PlaybackThread *playbackThread = (PlaybackThread *)thread.get(); *latencyMs = playbackThread-&gt;latency(); // notify client processes of the new output creation playbackThread-&gt;ioConfigChanged(AUDIO_OUTPUT_OPENED); // the first primary output opened designates the primary hw device if ((mPrimaryHardwareDev == NULL) &amp;&amp; (flags &amp; AUDIO_OUTPUT_FLAG_PRIMARY)) &#123; ALOGI("Using module %d as the primary audio interface", module); mPrimaryHardwareDev = playbackThread-&gt;getOutput()-&gt;audioHwDev; AutoMutex lock(mHardwareLock); mHardwareStatus = AUDIO_HW_SET_MODE; mPrimaryHardwareDev-&gt;hwDevice()-&gt;setMode(mMode); mHardwareStatus = AUDIO_HW_IDLE; &#125; &#125; else &#123; MmapThread *mmapThread = (MmapThread *)thread.get(); mmapThread-&gt;ioConfigChanged(AUDIO_OUTPUT_OPENED); &#125; return NO_ERROR; &#125; return NO_INIT;&#125; 我们可以看到在AudioPolicyManager中有mOutputs以audio_io_handle_t为key保存SwAudioOutputDescriptor，而在AudioFlinger中mPlaybackThreads和mMmapThreads以audio_io_handle_t保存线程，所以我们可以得出如下关系图。 打开音频输入打开音频输入和打开音频输出很类似，只是将SwAudioOutputDescriptor、PlaybackThread及AudioStreamOut换成了AudioInputDescriptor、RecordThread及AudioStreamIn，这里就一笔带过。 确保可用输入输出和默认输出真正可用无其他。主要是移除不可达设备。1234567891011121314151617181920212223242526272829303132333435363738// make sure all attached devices have been allocated a unique ID// 确保所有可用输出设备真正可用for (size_t i = 0; i &lt; mAvailableOutputDevices.size();) &#123; if (!mAvailableOutputDevices[i]-&gt;isAttached()) &#123; ALOGW("Output device %08x unreachable", mAvailableOutputDevices[i]-&gt;type()); mAvailableOutputDevices.remove(mAvailableOutputDevices[i]); continue; &#125; // The device is now validated and can be appended to the available // devices of the engine // 目前不做任何处理 mEngine-&gt;setDeviceConnectionState(mAvailableOutputDevices[i], AUDIO_POLICY_DEVICE_STATE_AVAILABLE); i++;&#125;// 确保所有可用输入设备真正可用for (size_t i = 0; i &lt; mAvailableInputDevices.size();) &#123; if (!mAvailableInputDevices[i]-&gt;isAttached()) &#123; ALOGW("Input device %08x unreachable", mAvailableInputDevices[i]-&gt;type()); mAvailableInputDevices.remove(mAvailableInputDevices[i]); continue; &#125; // The device is now validated and can be appended to the available devices of the engine // 目前不做任何处理 mEngine-&gt;setDeviceConnectionState(mAvailableInputDevices[i], AUDIO_POLICY_DEVICE_STATE_AVAILABLE); i++;&#125;// make sure default device is reachable// 确保默认输出设备真正可用if (mDefaultOutputDevice == 0 || mAvailableOutputDevices.indexOf(mDefaultOutputDevice) &lt; 0) &#123; ALOGE("Default device %08x is unreachable", mDefaultOutputDevice-&gt;type());&#125;ALOGE_IF((mPrimaryOutput == 0), "Failed to open primary output");updateDevicesAndOutputs(); AudioPolicyEffects初始化对于音效策略，类似会先加载audio_effects.conf，这个文件可能位于system/etc/或者vendor/etc/。123456789AudioPolicyEffects::AudioPolicyEffects()&#123; // load automatic audio effect modules if (access(AUDIO_EFFECT_VENDOR_CONFIG_FILE, R_OK) == 0) &#123; loadAudioEffectConfig(AUDIO_EFFECT_VENDOR_CONFIG_FILE); &#125; else if (access(AUDIO_EFFECT_DEFAULT_CONFIG_FILE, R_OK) == 0) &#123; loadAudioEffectConfig(AUDIO_EFFECT_DEFAULT_CONFIG_FILE); &#125;&#125; 这个文件的格式如下。123456789101112131415161718192021# List of effect libraries to load. Each library element must contain a "path" element# giving the full path of the library .so file.# libraries &#123;# &lt;lib name&gt; &#123;# path &lt;lib path&gt;# &#125;# &#125;# list of effects to load. Each effect element must contain a "library" and a "uuid" element.# The value of the "library" element must correspond to the name of one library element in the# "libraries" element.# The name of the effect element is indicative, only the value of the "uuid" element# designates the effect.# The uuid is the implementation specific UUID as specified by the effect vendor. This is not the# generic effect type UUID.# effects &#123;# &lt;fx name&gt; &#123;# library &lt;lib name&gt;# uuid &lt;effect uuid&gt;# &#125;# ...# &#125; 到这里AudioPolicyService的启动流程已经完结，且篇幅已经挺长，其他的知识点，学习到时再补上。]]></content>
      <categories>
        <category>Android Audio</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AudioFlinger启动过程]]></title>
    <url>%2F2017%2F11%2F09%2FAudioFlinger%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[AudioFlinger以media.audio_flinger为名注册到ServiceManager，是Android Audio系统的一个核心服务，是音频策略的执行者，负责输入输出流设备的管理及音频流数据的处理传输。本文以Android 8.0的代码为基础，记录了其启动过程以及AudioFlinger主要类的关系。其代码位于frameworks/av/services/audioflinger。 AudioFlinger的启动过程Android 8.0中与7.0相比，在初始化过程中，主要是实例化了mDevicesFactoryHal和mEffectsFactoryHal，作为HAL进程的客户端与HAL进程交互。12345678910111213141516171819202122232425AudioFlinger::AudioFlinger() //变量初始化 : BnAudioFlinger(), mMediaLogNotifier(new AudioFlinger::MediaLogNotifier()), mPrimaryHardwareDev(NULL), mAudioHwDevs(NULL), mHardwareStatus(AUDIO_HW_IDLE), mMasterVolume(1.0f), mMasterMute(false), // mNextUniqueId(AUDIO_UNIQUE_ID_USE_MAX), mMode(AUDIO_MODE_INVALID), mBtNrecIsOff(false), mIsLowRamDevice(true), mIsDeviceTypeKnown(false), mGlobalEffectEnableTime(0), mSystemReady(false)&#123; ...... mDevicesFactoryHal = DevicesFactoryHalInterface::create(); mEffectsFactoryHal = EffectsFactoryHalInterface::create(); ......&#125; 这里我们主要看一下DevicesFactoryHalInterface关系图。EffectsFactoryHalInterface也是类似的情况。阅读每个类的代码实现，我们发现只有DevicesFactoryHalHybrid实现了DevicesFactoryHalInterface接口的create()函数，所以这里会创建DevicesFactoryHalHybrid实例，而DevicesFactoryHalHybrid会创建DevicesFactoryHalLocal和DevicesFactoryHalHidl实例。DevicesFactoryHalLocal用于直接加载HAL的lib，是为了兼容8.0之前的版本，而DevicesFactoryHalHidl通过binder通信从hwservicemanager中返回IDevicesFactory实例，通过IDevicesFactory的openDevice函数返回具体的Device，并且用DeviceHalHidl封装返回的Device，这里不再会直接加载HAL的lib，后续和HAL的通信完全通过IDevicesFactory接口，具体在AudioPolicyService加载HW module时会更清楚明白。 第一次初始化还会执行onFirstRef()，创建PatchPanel实例且将AudioFlinger实例传入PatchPanel，设置Audio Mode为AUDIO_MODE_NORMAL,并将自己保存在全局变量gAudioFlinger。123456789101112void AudioFlinger::onFirstRef()&#123; Mutex::Autolock _l(mLock); ...... mPatchPanel = new PatchPanel(this); mMode = AUDIO_MODE_NORMAL; gAudioFlinger = this;&#125; Mutex是互斥类，用于多线程访问同一个资源的时候，保证一次只有一个线程能访问该资源。它的工作原理是某一个线程要访问公共资源的时候先锁定这个Mutex，完成操作之后对Mutex解锁，在此期间如果有其它的线程也要访问公共资源，它就先要去锁Mutex，当它发现Mutex已经被锁住了，那么这个线程就是阻塞在那儿。等Mutex解锁之后所有阻塞在Mutex的线程都会醒来，只有第一个醒来的会抢到Mutex，其它没有抢到的发现自己晚了一步，只能继续阻塞在那儿，等待下次机会。Mutex源码位置/system/core/libutils/include/utils。 为了简化一般的Mutex操作，在class Mutex中定义了一个内部类Autolock，它利用{}作用域实现自动解锁，看一下它的构造函数就知道了。12345678class Autolock &#123; public: inline explicit Autolock(Mutex&amp; mutex) : mLock(mutex) &#123; mLock.lock(); &#125; inline explicit Autolock(Mutex* mutex) : mLock(*mutex) &#123; mLock.lock(); &#125; inline ~Autolock() &#123; mLock.unlock(); &#125; private: Mutex&amp; mLock;&#125;; 我们知道在{}中创建的变量，变开这个大括号时就要销毁，于是就自动调用析构函数了。 AudioFlinger中放音录音线程AudioFlinger作为音频的核心服务，主要责任是负责放音与录音，下面我们可以大致看看放音与录音线程的关系，在AudioPolicyServic启动过程中我们会看到这些线程的创建。 ThreadBase：ThreadBase以Thread为基类，而又是PlaybackThread、RecordThread和MmapThread的基类。 RecordThread：音频录音线程，负责音频的录音，没有子类。 PlaybackThread：代表放音线程，有两个直接子类MixerThread和DirectOutputThread。 MixerThread：混音放音线程，有子类DuplicatingThread，负责处理标识为AUDIO_OUTPUT_FLAG_PRIMARY、AUDIO_OUTPUT_FLAG_FAST、AUDIO_OUTPUT_FLAG_DEEP_BUFFER的音频流，MixerThread 可以把多个音轨的数据混音后再输出。 DirectOutputThread：直接输出放音线程，有子类OffloadThread，负责处理标识为AUDIO_OUTPUT_FLAG_DIRECT的音频流，这种音频流数据不需要软件混音，直接输出到音频设备即可。 DuplicatingThread：复制输出放音线程，负责复制音频流数据到其他输出设备，使用场景如主声卡设备、蓝牙耳机设备、USB声卡设备同时输出。 OffloadThread：硬解回放线程，负责处理标识为AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD的音频流，这种音频流未经软件解码的（一般是MP3、AAC等格式的数据），需要输出到硬件解码器，由硬件解码器解码成PCM 数据。 MmapThread：这个线程是Android 8.0新加入的，用于低延迟的放音与录音，与AAudio有关系，有MmapPlaybackThread和MmapCaptureThread两个子类。 MmapPlaybackThread：MMAP放音线程，负责处理标识为AUDIO_OUTPUT_FLAG_MMAP_NOIRQ的音频流。 MmapcaptureThread：MMAP录音线程，负责处理标识为AUDIO_INPUT_FLAG_MMAP_NOIRQ的音频流。AudioFlinger中TracksTrack：音轨，是AudioFlinger中另一个重要的将角色，下面我们可以看看其关系。对于播放对应Track，OutputTrack，TrackHandle及BnAudioTrack，TrackHandle和BnAudioTrack主要用于和Client端Binder通信，真正代表输出音轨的为Track。同样对应录音音轨的是RecordTrack，RecordHandle及BnAudioRecord，RecordHandle和BnAudioRecord也用于Binder通信，真正录音音轨为RecordTrack。而对于MmapTrack稍微不太样，而是定义通用接口MmapStreamInterface封装MmapThread，再封装MmapTrack，不是通过XXXHandle继承BnXXX，这也许是出于latency上的考虑。AudioFlinger中的Streams我们看到在AudioFlinger中以AudioHwDevice封装HAL的DeviceHalHidl，而DeviceHalHidl封装从HAL返回的具体的Device，这就和HAL层的so文件连接在一起，且AudioHwDevice直接或间接依赖AudioStreamOut和AudioStreanIn这样也和HAL层中stream关联上，后续打开音频输入输出以及打开输入音频流及输出音频流做好准备。具体我们可以在AudioPolicyService启动的时候看的更清楚。AudioFlinger中还有很多其他的知识点，后续学到时再慢慢补上。]]></content>
      <categories>
        <category>Android Audio</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Audio Server启动]]></title>
    <url>%2F2017%2F11%2F08%2FAndroid-Audio-Server%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Android audioserver是Audio系统native的服务，也是连接Audio Framework和Audio HAL的一个纽带，其中包含了AudioFlinger、AudioPolicyService、AAudioService、RadioService、SoundTriggerHwService等服务。源代码位于frameworks/av/media/audioserver 从Android 7.0开始，Audio相关的service从mediaserver中转移到audioserver，把audio，camera及mediaplayerservice做了一个拆分，这样不会显得臃肿、职能更加独立且安全性更高。拆分之后audioserver还是一个native service，还是从init进程中启动，如下是其在audioserver.rc中的启动代码。 12345678910service audioserver /system/bin/audioserver class main # audioserver和class main行为一致 user audioserver # 用户归属，uid：AID_AUDIOSERVER # media gid needed for /dev/fm (radio) and for /data/misc/media (tee) group audio camera drmrpc inet media mediadrm net_bt \ net_bt_admin net_bw_acct oem_2901 # 用户组归属 ioprio rt 4 # io调度优先级 # 当子进程被创建的时候，将子进程的pid写入到给定的文件中,cgroup/cpuset用法 writepid /dev/cpuset/foreground/tasks /dev/stune/foreground/tasks onrestart restart audio-hal-2-0 # audioserver重启会重启hal 我们看到Android 8.0当重启audioserver时，会重启audio-hal-2-0，这个服务是audio hal的服务，在android 8.0中，framework native进程与hal分离，hal不在和framework native处于同一个进程，而是独立进程，进程间通过binder通信。先不讲HAL binder化，我们先看看audioserver中包含哪几个服务。123456789101112131415161718192021222324252627282930313233int main(int argc __unused, char **argv)&#123; signal(SIGPIPE, SIG_IGN); bool doLog = (bool) property_get_bool("ro.test_harness", 0); pid_t childPid; ...... if (doLog &amp;&amp; (childPid = fork()) != 0) &#123; ...... &#125; else &#123; sp&lt;ProcessState&gt; proc(ProcessState::self()); sp&lt;IServiceManager&gt; sm = defaultServiceManager(); ALOGI("ServiceManager: %p", sm.get()); AudioFlinger::instantiate(); AudioPolicyService::instantiate(); AAudioService::instantiate(); RadioService::instantiate(); SoundTriggerHwService::instantiate(); ProcessState::self()-&gt;startThreadPool();// FIXME: remove when BUG 31748996 is fixed android::hardware::ProcessState::self()-&gt;startThreadPool(); IPCThreadState::self()-&gt;joinThreadPool(); &#125;&#125; 从如上代码可以看出，audioserver中回依次执行AudioFlinger、AudioPolicyService、AAudioService、RadioService、SoundTriggerHwService的instantiate函数。通过阅读源代码，由于继承的缘故这个五个service最终会调用BinderService的instantiate函数且将自己注册到ServiceManager中，后续client端可以通过service注册时用的name从ServiceManager返回server端。12345678910111213141516171819202122232425262728template&lt;typename SERVICE&gt;class BinderService&#123;public: static status_t publish(bool allowIsolated = false) &#123; sp&lt;IServiceManager&gt; sm(defaultServiceManager()); return sm-&gt;addService( String16(SERVICE::getServiceName()), new SERVICE(), allowIsolated); &#125; static void publishAndJoinThreadPool(bool allowIsolated = false) &#123; publish(allowIsolated); joinThreadPool(); &#125; static void instantiate() &#123; publish(); &#125; static status_t shutdown() &#123; return NO_ERROR; &#125;private: static void joinThreadPool() &#123; sp&lt;ProcessState&gt; ps(ProcessState::self()); ps-&gt;startThreadPool(); ps-&gt;giveThreadPoolName(); IPCThreadState::self()-&gt;joinThreadPool(); &#125;&#125;; AudioFlinger（media.audio_flinger）：Audio系统的一个核心服务，是音频策略的执行者，负责输入输出流设备的管理及音频流数据的处理传输。 AudioPolicyService（media.audio_policy）：音频策略的制定者，负责音频设备切换的策略抉择、音量调节策略等。 AAudioService（media.aaudio）：这是Android 8.0加入角色，是OpenSL ES的另外一种选择，需要低延迟的高性能音频应用的另外一种选择。 RadioService（media.radio）：与FM相关的一个服务。 SoundTriggerHwService（media.sound_trigger_hw）：Android语音识别的native服务。]]></content>
      <categories>
        <category>Android Audio</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub搭建个人的博客]]></title>
    <url>%2F2017%2F10%2F30%2FGitHub%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[最近突然间发现，自己过去看过的东西，没过多久就忘得一干二净，每当要用时，又得从头来一遍，真是好记性不如烂笔头，所以有了写笔记的想法，GitHub上可以方便记录自己一切想记录的，于是就在GitHub上开始写笔记，希望自己不要把知识忘得太快。本文记录在Windows环境下使用Hexo搭建GitHub博客的过程。 安装Node.js在nodejs官网下载对应的版本安装下载地址：https://nodejs.org/en/download 安装Git下载地址：https://git-for-windows.github.io 创建GitHub账户进入GitHub主页https://github.com/，依次输入用户名、邮箱、密码，然后点击注册，按默认点击“Finish sign up”。然后进行邮箱验证。 创建GitHub仓库点击“New repository”，新建一个仓库，仓库名为“[yourname].github.io”，这样https://[yourname].github.io 就是你的博客地址了。默认这仓库只有master分支，新建一个hexo分支。 配置Hexo接下来的命令都在Git Bash中执行。 在自己喜欢的位置新建一个blog文件夹，在这个文件夹下打开Git Bash，因为npm是国外服务器，可能执行比较慢，可以使用淘宝镜像，命令如下： $ npm install -g cnpm --registry=https://registry.npm.taobao.org 执行成功后使用淘宝NPM安装Hexo $ cnpm install -g hexo-cli $ cnmp install hexo --save $ hexo -v hexo: 3.4.0 hexo-cli: 1.0.4 os: Windows_NT 6.1.7600 win32 ia32 http_parser: 2.7.0 node: 8.7.0 v8: 6.1.534.42 uv: 1.15.0 zlib: 1.2.11 ares: 1.10.1-DEV modules: 57 nghttp2: 1.25.0 openssl: 1.0.2l icu: 59.1 unicode: 9.0 cldr: 31.0.1 tz: 2017b 到这里hexo已经安装好了 配置ssh keyssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; 在C:\Users\yourname\.ssh下会得到密钥id_rsa和id_rsa.pub，用nodepad++打开id_rsa.pub复制全文，打开https://github.com/settings/ssh，Add SSH key，粘贴进去保存。 测试是否配置成功 $ ssh -T git@github.com Hi [yourname]! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 配置git信息$ git config --global user.name &quot;你的用户名&quot; $ git config --global user.email &quot;你的邮箱&quot; 使用Hexo管理博客初始化博客$ hexo init &lt;nodejs-hexo&gt; //初始化nodejs-hexo（文件夹名随意） $ git clone -b hexo https://github.com/[yourname]/[yourname].github.io 将[yourname].github.io文件夹下的.git文件夹拷贝到nodejs-hexo文件夹。 $ cnpm install //安装生成器 $ hexo server //运行（Ctrl + C停止运行） 在浏览器输入localhost:4000，这样就可以在本地看到博客了。 配置博客_config.yml中配置基本信息 title: #博客标题 subtitle: #博客副标题 description: #博客描述 author: #博客作者 language: zh-Hans timezone: Asia/Shanghai _config.yml中配置主题 theme: next _config.yml中配置部署 deploy: type: git repo: https://github.com/[yourname]/[yourname].github.io branch: master 注意：这里的设置冒号后面必须有空格 发布博客$ hexo new &quot;博客名&quot; //增加新文章 $ cnpm install hexo-deployer-git --save //安装hexo git插件 $ git add . $ git commit -m &quot;message&quot; $ git push origin hexo $ hexo generate //生成静态文件 $ hexo deploy //部署 换PC管理博客$ git clone -b hexo https://github.com/[yourname]/[yourname].github.io 在[yourname].github.io中从新安装hexo，就可以写博客及发布博客了。 参考Next主题配置参考：http://theme-next.iissnan.com/theme-settings.html]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
